{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train and test data\n",
    "\n",
    "meta_train_df = pd.read_csv(\"review_meta_train.csv\")\n",
    "text_train_df = pd.read_csv(\"review_text_train.csv\")\n",
    "\n",
    "train_df = pd.concat([text_train_df, meta_train_df], axis=1)\n",
    "\n",
    "meta_test_df = pd.read_csv(\"review_meta_test.csv\")\n",
    "text_test_df = pd.read_csv(\"review_text_test.csv\")\n",
    "\n",
    "test_df = pd.concat([text_test_df, meta_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert read in correctly\n",
    "\n",
    "assert(len(train_df) == 28068)\n",
    "assert(len(test_df) == 7018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explore the metadata\n",
    "- try a zeroR model and try some simple models using only the metadata ( they do not outperform zeroR )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6871882570899245\n"
     ]
    }
   ],
   "source": [
    "value_counts = train_df['rating'].value_counts()\n",
    "zeroR_accuracy = value_counts.values.max()/value_counts.sum()\n",
    "print(zeroR_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try a quick simple NB model using the voting meta data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>vote_funny</th>\n",
       "      <th>vote_cool</th>\n",
       "      <th>vote_useful</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear longman &amp; eagle.......you've left me no c...</td>\n",
       "      <td>3/15/2012</td>\n",
       "      <td>-s77HISu8DVQ8F0HxmWW6A</td>\n",
       "      <td>mthr7h15a_z9m9jRI6mG6Q</td>\n",
       "      <td>m5_GCJP2W4zEJnyVgxa3eA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delish. The hubby and I wanted to do brunch on...</td>\n",
       "      <td>6/21/2010</td>\n",
       "      <td>A2aCzGCgg6gAbatHiCrPfA</td>\n",
       "      <td>rhM01fl3iU0xHr3TIpCMhQ</td>\n",
       "      <td>m5_GCJP2W4zEJnyVgxa3eA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yep, I've giving Yolk 5 stars. It's just reall...</td>\n",
       "      <td>7/29/2011</td>\n",
       "      <td>DK2pd</td>\n",
       "      <td>SNHKDgmGiLn5chUlhdLCkg</td>\n",
       "      <td>CwPi6NVuJIZZx4IBcTekFQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meat, meat, meat. It's meat-tastic. So much me...</td>\n",
       "      <td>3/10/2006</td>\n",
       "      <td>b3BkUiWJEKNQko</td>\n",
       "      <td>HXjk1RVfLMPeZxitnk1Auw</td>\n",
       "      <td>43rd1LKcZRIunySzbMsyLQ</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I caught up with the law school girls on a Sat...</td>\n",
       "      <td>8/28/2012</td>\n",
       "      <td>RabHhte</td>\n",
       "      <td>W0ny0BqO0OJ4K4aVnSIlBw</td>\n",
       "      <td>CwPi6NVuJIZZx4IBcTekFQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review       date  \\\n",
       "0  dear longman & eagle.......you've left me no c...  3/15/2012   \n",
       "1  Delish. The hubby and I wanted to do brunch on...  6/21/2010   \n",
       "2  yep, I've giving Yolk 5 stars. It's just reall...  7/29/2011   \n",
       "3  Meat, meat, meat. It's meat-tastic. So much me...  3/10/2006   \n",
       "4  I caught up with the law school girls on a Sat...  8/28/2012   \n",
       "\n",
       "                review_id             reviewer_id             business_id  \\\n",
       "0  -s77HISu8DVQ8F0HxmWW6A  mthr7h15a_z9m9jRI6mG6Q  m5_GCJP2W4zEJnyVgxa3eA   \n",
       "1  A2aCzGCgg6gAbatHiCrPfA  rhM01fl3iU0xHr3TIpCMhQ  m5_GCJP2W4zEJnyVgxa3eA   \n",
       "2                   DK2pd  SNHKDgmGiLn5chUlhdLCkg  CwPi6NVuJIZZx4IBcTekFQ   \n",
       "3          b3BkUiWJEKNQko  HXjk1RVfLMPeZxitnk1Auw  43rd1LKcZRIunySzbMsyLQ   \n",
       "4                 RabHhte  W0ny0BqO0OJ4K4aVnSIlBw  CwPi6NVuJIZZx4IBcTekFQ   \n",
       "\n",
       "   vote_funny  vote_cool  vote_useful  rating  \n",
       "0           0          1            3       1  \n",
       "1           0          0            0       5  \n",
       "2           1          0            1       5  \n",
       "3          17          3            3       3  \n",
       "4           0          0            0       3  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['vote_funny', 'vote_cool', 'vote_useful']\n",
    "labels = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_df = train_df[features + labels].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67438546 0.67491984 0.66512291 0.67967219 0.67824693]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(gnb, voting_df[features], voting_df['rating'].ravel())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this does worse than zeroR, probably there arent enough examples of each particular amount of votes/0 votes is most prevalent so it does about as well as predicting the most common class. lets try transforming each number of votes to a percentage of total votes and see how it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_df['total_votes'] = voting_df['vote_funny'] + voting_df['vote_cool'] + voting_df['vote_useful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_percentage = feature + \"_percentage\"\n",
    "    voting_df[feature_percentage] = voting_df[feature]/voting_df['total_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote_funny</th>\n",
       "      <th>vote_cool</th>\n",
       "      <th>vote_useful</th>\n",
       "      <th>rating</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vote_funny_percentage</th>\n",
       "      <th>vote_cool_percentage</th>\n",
       "      <th>vote_useful_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0.73913</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vote_funny  vote_cool  vote_useful  rating  total_votes  \\\n",
       "0           0          1            3       1            4   \n",
       "1           0          0            0       5            0   \n",
       "2           1          0            1       5            2   \n",
       "3          17          3            3       3           23   \n",
       "4           0          0            0       3            0   \n",
       "\n",
       "   vote_funny_percentage  vote_cool_percentage  vote_useful_percentage  \n",
       "0                0.00000              0.250000                0.750000  \n",
       "1                    NaN                   NaN                     NaN  \n",
       "2                0.50000              0.000000                0.500000  \n",
       "3                0.73913              0.130435                0.130435  \n",
       "4                    NaN                   NaN                     NaN  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = [feature + \"_percentage\" for feature in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_df[new_features] = voting_df[new_features].fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67901674 0.67581047 0.67687923 0.67610903 0.67575272]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(gnb, voting_df[new_features], voting_df['rating'].ravel())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNB is probably not the correct model to use on this data as P(percentage_funny) * P(percentage_cool) != P(percentage_funny and percentage_cool)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe we can try a tree based model to see if the meta data is helpful in classification... this will also deal better with the nan values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68721055 0.68578554 0.69006056 0.68537324 0.68608587]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, voting_df[new_features], voting_df['rating'].ravel(), cv = 5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest provides a similar performance... using the meta data alone seems to be no good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try classifying based on net sentiment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment lexicon\n",
    "\n",
    "http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv(\"AFINN/AFINN-111.txt\", header = None)\n",
    "lexicon['word'] = lexicon[0].apply(lambda x: x.split(\"\\t\")[0])\n",
    "lexicon['sentiment'] = lexicon[0].apply(lambda x: x.split(\"\\t\")[1])\n",
    "lexicon = lexicon.drop(0, axis = 1)\n",
    "lexicon['sentiment'] = pd.to_numeric(lexicon['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>yucky</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>yummy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>zealot</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>zealots</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>zealous</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2477 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  sentiment\n",
       "0       abandon         -2\n",
       "1     abandoned         -2\n",
       "2      abandons         -2\n",
       "3      abducted         -2\n",
       "4     abduction         -2\n",
       "...         ...        ...\n",
       "2472      yucky         -2\n",
       "2473      yummy          3\n",
       "2474     zealot         -2\n",
       "2475    zealots         -2\n",
       "2476    zealous          2\n",
       "\n",
       "[2477 rows x 2 columns]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_sentiment(txt, lexicon = lexicon):\n",
    "    s = txt.lower()\n",
    "    words = re.sub(r'[^\\w\\s]|(\\n)','',s).split(\" \")\n",
    "    words_df = pd.DataFrame(data = {\"word\": words})\n",
    "    words_df = words_df.merge(lexicon, how = \"left\", on = \"word\").fillna(0)\n",
    "    sentiment = words_df['sentiment'].sum()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment(train_df.review[0], lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = list(map(get_sentiment, X_train_txt.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(sentiment).reshape(-1, 1), train_df['rating'].ravel().reshape(-1, 1), train_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "clf =  RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5301\n",
       "1     309\n",
       "3       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(clf.predict(X_test)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.70555753, 0.70502316, 0.70751692, 0.70586139, 0.70550508])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, np.array(sentiment).reshape(-1, 1), train_df['rating'].ravel().reshape(-1, 1), cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [5],\n",
       "       [5],\n",
       "       ...,\n",
       "       [5],\n",
       "       [5],\n",
       "       [5]], dtype=int64)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['rating'].ravel().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class labels are quite unbalanced. the models might be getting lazy and simply picking the majority class most of the time.\n",
    "what can we do about this? \n",
    "lets try building some samples with balanced classes using bootstrapping for example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(sentiment).reshape(-1, 1), train_df['rating'].ravel().reshape(-1, 1), train_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rus, y_rus = rus.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier().fit(X_rus, y_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46954043462771644"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2e3af506908>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAX6klEQVR4nO3df5AcZ33n8fdH47FZGccro72LvJIs8HGuCsi2zJQll+4oCqjYFo6l+Ecs1ynYJEGVnElMJecEBwo4l+9c3FZ8kPNVXCJQZ4Mi4GyjU1xQwhwQQipSMvphyY7QReZM9OvQYkcyLi9itfreH9Mrj2Z7dnp2Z3dGD59X1dT2dD/T/VXP05/t7XlGrYjAzMzSM6fbBZiZ2cxwwJuZJcoBb2aWKAe8mVmiHPBmZok6r1sbnj9/fixZsqRbmzczOyft2LHjxxExUKRt1wJ+yZIlVKvVbm3ezOycJOmHRdv6Eo2ZWaIc8GZmiXLAm5klygFvZpYoB7yZWaIc8GZmiSo0TFLSi8BPgDHgVERUGpYL+AywCngNuDsidna21OI27zrM0Nb9HDk+wqX9fdx3/RWsWTbYtToOHx+Z9W2b5REw1f8/tr+vzM9OjfHa6GkA5s0t874rF/D0s0c5PjIKwBzB6YALzy/x2s/GJmxrfPslibEIBnOOz7zjF5jWMd0sE2Y7K2Z7eyry3wVnAV+JiB83Wb4K+F1qAb8c+ExELJ9snZVKJWZiHPzmXYe5/6m9jIyOnZnXVy7x0C1LZzXk8+ows4nqj8+846ZcEgSMno7c17TSLBNufccgT+44PGtZ0alskrSj8SS7mU5dolkNPB4124B+SQs6tO62DG3dPyFUR0bHGNq6v+t1mNlE9cdn3nEzOhZnhXvja1pplgmbth+c1azoRjYVDfgAviFph6T1OcsHgYN1zw9l884iab2kqqTq8PBw+9UWcKTJ5ZBm82fKbG/P7Fw2fry0c9wUbdus3ViTqxczdex2I5uKBvzKiLgGuBG4R9I7G5Yr5zUT9l5EbIiISkRUBgYK/VcKbbu0v6+t+TNltrdndi4bP17aOW6Ktm3WrqS82Jq5Y7cb2VQo4CPiSPbzGPBV4NqGJoeARXXPFwJHOlFgu+67/gr6yqWz5vWVS2c+qOlmHWY2Uf3xmXfclEuiPEdNX9NKs0y4c/miWc2KbmRTy4CXdKGki8angV8GnmtotgV4v2pWACci4mjHqy1gzbJBHrplKYP9fQgY7O+b9Q9YG+sw6xX556zF9PeVmVt+PTLmzS2zbsVi+vvKZ+aN5/CF55dytzU+b/zsufH4zDt+h267iqHbr5ryMd0sEx5cs3RWs6Ib2dRyFI2kt1A7a4fasMq/iIj/JOm3ASLi0WyY5CPADdSGSX4gIiYdIjNTo2jMzFLWziialuPgI+IHwFU58x+tmw7gnnaKNDOzmeVvspqZJcoBb2aWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiCge8pJKkXZKezll2t6RhSbuzx291tkwzM2tXyxt+1LkX2Af8QpPlX46ID02/JDMz64RCZ/CSFgLvA/58ZssxM7NOKXqJ5tPAHwKnJ2lzq6Q9kp6QtCivgaT1kqqSqsPDw+3WamZmbWgZ8JJuAo5FxI5Jmv0lsCQirgS+CTyW1ygiNkREJSIqAwMDUyrYzMyKKXIGvxK4WdKLwJeAd0v6Yn2DiHgpIk5mTz8LvKOjVZqZWdtaBnxE3B8RCyNiCbAW+FZErKtvI2lB3dObqX0Ya2ZmXdTOKJqzSHoAqEbEFuD3JN0MnAJeBu7uTHlmZjZVioiubLhSqUS1Wu3Kts3MzlWSdkREpUhbf5PVzCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLVOGAl1SStEvS0znLLpD0ZUkHJG2XtKSTRZqZWfvaOYO/l+b3Wv1N4J8j4l8B/xX41HQLMzOz6SkU8JIWAu8D/rxJk9XAY9n0E8B7JGn65ZmZ2VQVPYP/NPCHwOkmyweBgwARcQo4AbypsZGk9ZKqkqrDw8NTKNfMzIpqGfCSbgKORcSOyZrlzJtwN++I2BARlYioDAwMtFGmmZm1q8gZ/ErgZkkvAl8C3i3piw1tDgGLACSdB1wMvNzBOs3MrE0tAz4i7o+IhRGxBFgLfCsi1jU02wLclU3flrWZcAZvZmaz57ypvlDSA0A1IrYAnwO+IOkAtTP3tR2qz8zMpqitgI+I7wDfyaY/Xjf/p8DtnSzMzMymx99kNTNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0QVuSfrGyT9naRnJT0v6T/mtLlb0rCk3dnjt2amXDMzK6rIDT9OAu+OiFcllYHvSfp6RGxraPfliPhQ50s0M7OpaBnw2b1VX82elrOH77dqZtbjCl2Dl1SStBs4BjwTEdtzmt0qaY+kJyQt6miVZmbWtkIBHxFjEXE1sBC4VtLbG5r8JbAkIq4Evgk8lrceSeslVSVVh4eHp1O3mZm10NYomog4Tu2m2zc0zH8pIk5mTz8LvKPJ6zdERCUiKgMDA1Mo18zMiioyimZAUn823Qe8F/h+Q5sFdU9vBvZ1skgzM2tfkVE0C4DHJJWo/UL4SkQ8LekBoBoRW4Dfk3QzcAp4Gbh7pgo2M7NiVBskM/sqlUpUq9WubNvM7FwlaUdEVIq09TdZzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRLW8o5OkNwDfBS7I2j8REZ9oaHMB8Di1e7G+BNwRES92vNoO+NjmvWzc9k/U3+Zk3YrFPLhmadPXbN51mKGt+zlyfIT+uWV+OjrGyOhpAObNLfO+Kxfw7e8Pc+T4CJf293Hf9VewZtngWa97Q3kOJ0+d5nR37q9iPWLe3DIRcGJkNLdPDNb1n49t3sum7QcZq7spT39fGQn++bXRs9Y7tzyH/3zLlVR/+PKZ15Qk7ly+CGDCvMpll5zpm+N9FuCTW57n+MjomVqb9e121B8HU11HN9adgpZ3dJIk4MKIeFVSGfgecG9EbKtr8++BKyPityWtBX41Iu6YbL3duKPTxzbv5Yvb/il3WbOQ37zrMPc/tZeR0bHC2+krl7j1HYM8ueNwW68zg1r/uWbxxfzNCy/P2DbmAKfrnpdLYmwszprXrLaHbllaOETzjp9219GNdfeyjt7RKWpezZ6Ws0fjb4XVwGPZ9BPAe7JfDD1l0/aDbS8b2rq/7ZAeGR1j0/aDDnebkpHRsRkNd2BCkI8WCHeo1Ta0dX/h7eQdP+2uoxvrTkWha/CSSpJ2A8eAZyJie0OTQeAgQEScAk4Ab8pZz3pJVUnV4eHh6VU+BWOT/LXSbNmR4yMd35bZuaydY6JZ26keV7O17lQUCviIGIuIq4GFwLWS3t7QJO9sfULCRcSGiKhERGVgYKD9aqepNMkfFc2WXdrf1/FtmZ3L2jkmmrWd6nE1W+tORVujaCLiOPAd4IaGRYeARQCSzgMuBmb2b8wpGP/AqZ1l911/BX3lUlvb6SuXuHP5orZfZwa1/rPy8ktmdBuNB365pEJh0FcunflAtoi846fddXRj3alo+Z5KGpDUn033Ae8Fvt/QbAtwVzZ9G/CtaPXpbRc8uGYp61YsnvDnxmSjaNYsG+ShW5Yy2N+HqI0s6Cu/vtvmzS2zbsXiM8sH+/t46JalPLhm6Vmv6yvPYY5P6n/uzZtbro2EIb9PjPefjR+8jnUrFk/4S7C/r8y8ueUJ651bnsOn77j6rNeUJNatWJw77+E7rj6rzw7ddhUP33E1/X2vr7tZ327nA8zG42cq6+jGulNRZBTNldQ+QC1R+4XwlYh4QNIDQDUitmRDKb8ALKN25r42In4w2Xq7MYrGzOxc184ompbj4CNiD7Xgbpz/8brpnwK3t1OkmZnNLH+T1cwsUQ54M7NEOeDNzBLlgDczS5QD3swsUQ54M7NEOeDNzBLlgDczS5QD3swsUQ54M7NEOeDNzBLlgDczS5QD3swsUQ54M7NEOeDNzBJV5I5OiyR9W9I+Sc9LujenzbsknZC0O3t8PG9dZmY2e1re8AM4BfxBROyUdBGwQ9IzEfEPDe3+OiJu6nyJZmY2FS3P4CPiaETszKZ/AuwDfNNDM7Me19Y1eElLqN2+b3vO4uskPSvp65Le1oHazMxsGopcogFA0huBJ4EPR8QrDYt3ApdFxKuSVgGbgbfmrGM9sB5g8eLFUy7azMxaK3QGL6lMLdw3RsRTjcsj4pWIeDWb/hpQljQ/p92GiKhERGVgYGCapZuZ2WSKjKIR8DlgX0Q83KTNL2btkHRttt6XOlmomZm1p8glmpXArwN7Je3O5v0xsBggIh4FbgN+R9IpYARYGxExA/WamVlBLQM+Ir4HqEWbR4BHOlWUmZlNn7/JamaWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiHPBmZolywJuZJcoBb2aWKAe8mVmiHPBmZolywJuZJarILfsWSfq2pH2Snpd0b04bSfpTSQck7ZF0zcyUa2ZmRRW5Zd8p4A8iYqeki4Adkp6JiH+oa3Mj8NbssRz4s+ynmZl1Scsz+Ig4GhE7s+mfAPuAwYZmq4HHo2Yb0C9pQcerNTOzwtq6Bi9pCbAM2N6waBA4WPf8EBN/CSBpvaSqpOrw8HB7lZqZWVsKB7ykNwJPAh+OiFcaF+e8JCbMiNgQEZWIqAwMDLRXqZmZtaVQwEsqUwv3jRHxVE6TQ8CiuucLgSPTL8/MzKaqyCgaAZ8D9kXEw02abQHen42mWQGciIijHazTzMzaVGQUzUrg14G9knZn8/4YWAwQEY8CXwNWAQeA14APdL5UMzNrR8uAj4jvkX+Nvb5NAPd0qigzM5s+f5PVzCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLVJFb9n1e0jFJzzVZ/i5JJyTtzh4f73yZZmbWriK37PsfwCPA45O0+euIuKkjFZmZWUe0PIOPiO8CL89CLWZm1kGdugZ/naRnJX1d0tuaNZK0XlJVUnV4eLhDmzYzszydCPidwGURcRXw34DNzRpGxIaIqEREZWBgoAObNjOzZqYd8BHxSkS8mk1/DShLmj/tyszMbFqmHfCSflGSsulrs3W+NN31mpnZ9LQcRSNpE/AuYL6kQ8AngDJARDwK3Ab8jqRTwAiwNiJixio2M7NCWgZ8RNzZYvkj1IZRmplZD/E3Wc3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRRW748XngJuBYRLw9Z7mAzwCrgNeAuyNiZ6cLBdi86zBDW/dz5PgIl/b3cd/1VwBMmLdm2WDb6/53n/1b/uaFl888X3n5JWz84HVntvtHT+7h5KnTE1534fklTo6Occq3OJlVAgIYzOkH/XPLRMCJkdEJ/eTw8ZEzrwWYN7fMJ37lbWe9vr4ffWzzXjZtP8hYBCWJO5cv4sE1Szv+78nr21Ppx2b11OrmS5LeCbwKPN4k4FcBv0st4JcDn4mI5a02XKlUolqtFi50867D3P/UXkZGx87MK88RCEbHXv839JVLPHTL0rYOjsZwH7fy8ku4vbKY3//Kbk47wHtWuSQIGG3yJuX1k3pzBKU5mtCPrll8cW6/WLdicUdDPq9vT6Uf288HSTsiolKkbctLNBHxXWBiL3/damrhHxGxDeiXtKBYqcUNbd1/1gEAtQO68aAdGR1jaOv+ttaddxCPzx/aut/h3uNGx6JpuEN+P6l3OiaG/8joWNN+sWn7wakV2kRe355KPzZr1Ilr8INAfY8/lM2bQNJ6SVVJ1eHh4bY2cuT4yIy0nc11WRrGOnzL4WZ9zH3PpqsTAa+ceblHQERsiIhKRFQGBgba2sil/X0z0nY212VpKCmvy09dsz7mvmfT1YmAPwQsqnu+EDjSgfWe5b7rr6CvXDprXnmOatdf6/SVS2c+VCtq5eWXNJ1/3/VXMKezx7N1WLmk2nX2Zstz+km9OSK3HzXrF3cuX5Q7f6ry+vZU+rFZo04E/Bbg/apZAZyIiKMdWO9Z1iwb5KFbljLY34eojZ4Yuv0qhm676qx5U/lgauMHr5twMI+PolmzbJCHf+1qLjgvf1ddeH6J8/wLYNaN7/LB/j6GbruKodtf7wfz5pbp7yvn9pP615K1ffjXrs7tRxs/eB3rViw+c8Zekjr+ASvk921/wGqdUGQUzSbgXcB84EfAJ4AyQEQ8mg2TfAS4gdowyQ9ERMvhMe2OojEzs/ZG0bQcBx8Rd7ZYHsA9BWszM7NZ4m+ympklygFvZpYoB7yZWaIc8GZmiXLAm5klquUwyRnbsDQM/DBn0Xzgx7NcTjt6vT7o/Rp7vT7o/Rp7vT7o/RrP1foui4hC/xVA1wK+GUnVomM8u6HX64Per7HX64Per7HX64Per/HnoT5fojEzS5QD3swsUb0Y8Bu6XUALvV4f9H6NvV4f9H6NvV4f9H6NydfXc9fgzcysM3rxDN7MzDrAAW9mlqieCHhJn5R0WNLu7LGqbtn9kg5I2i/p+i7WOCTp+5L2SPqqpP5s/hJJI3W1P9rFGm/I9tMBSR/pVh31JC2S9G1J+yQ9L+nebH7T97wLNb4oaW9WRzWbd4mkZyT9Y/ZzXhfru6JuP+2W9IqkD3dzH0r6vKRjkp6rm5e7z7J7Rfxp1i/3SLqmizX2zHHcpL7OZmFEdP0BfBL4Dznzfwl4FrgAeDPwAlDqUo2/DJyXTX8K+FQ2vQR4rgf2YSnbP28Bzs/22y/1QF0LgGuy6YuA/5O9r7nveZdqfBGY3zDvvwAfyaY/Mv5+d/uRvc//D7ism/sQeCdwTX3fb7bPgFXA16nda2UFsL2LNfbMcdykvo5mYU+cwU9iNfCliDgZEf8XOABc241CIuIbEXEqe7qN2q0Je8m1wIGI+EFE/Az4ErX911URcTQidmbTPwH20eSm7D1mNfBYNv0YsKaLtdR7D/BCROR9C3zWRMR3gZcbZjfbZ6uBx6NmG9AvaUE3auyl47jJPmxmSlnYSwH/oezPps/X/Tk8CBysa3OI3giH36B2RjLuzZJ2SforSf+2SzX16r46Q9ISYBmwPZuV9553QwDfkLRD0vps3r+M7NaT2c9/0bXqzrYW2FT3vFf2ITTfZ73aN3vxOIYOZuGsBbykb0p6LuexGvgz4HLgauAo8CfjL8tZ1YyN62xR43ibjwKngI3ZrKPA4ohYBvw+8BeSfmGmapzErO6rdkl6I/Ak8OGIeIXm73k3rIyIa4AbgXskvbOLtTQl6XzgZuB/ZrN6aR9Opuf6Zg8fxx3Nwpa37OuUiHhvkXaSPgs8nT09BNTfwn4hcKTDpZ3RqkZJdwE3Ae+J7MJYRJwETmbTOyS9APxrYLZvODur+6odksrUwn1jRDwFEBE/qlte/57Puog4kv08Jumr1P70/ZGkBRFxNLuccKxb9dW5Edg5vu96aR9mmu2znuqbvXwcT/KeTmkf9sQlmobrcb8KjH+qvAVYK+kCSW8G3gr83WzXB7URKsAfATdHxGt18wcklbLpt2Q1/qALJf498FZJb87O9NZS239dJUnA54B9EfFw3fxm7/msknShpIvGp6l9CPcctX13V9bsLuB/daO+BndSd3mmV/ZhnWb7bAvw/mw0zQrgxPilnNnW68dxx7NwNj81nuTT5C8Ae4E92T9kQd2yj1L7xHg/cGMXazxA7RrY7uzxaDb/VuB5ap9w7wR+pYs1rqI2SuUF4KPdfl+zmv4NtT8l99Ttu1WTveezXN9bsvfu2ex9/Gg2/03A/wb+Mft5SZf341zgJeDiunld24fUftEcBUapnV3+ZrN9Ru3ywn/P+uVeoNLFGnvmOG5SX0ez0P9VgZlZonriEo2ZmXWeA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRP1/u/KrIIP79zoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train_df.sentiment, train_df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1     1.771404\n",
       "3    11.965860\n",
       "5    14.933171\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['rating'])['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 1 ratings have a super low sentiment so why the fuck arent my classifiers doing a good job!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = KMeans(n_clusters = 3, n_init = 100, max_iter = 100000, verbose = 1)\n",
    "# clf.fit(train_df.sentiment.ravel().reshape(-1, 1))\n",
    "# clf.fit(train_df.sentiment.ravel().reshape(-1, 1))\n",
    "# clusters = clf.labels_\n",
    "# clf.cluster_centers_\n",
    "# pd.Series(clf.labels_).value_counts()\n",
    "# train_df['clusters'] = clusters\n",
    "# train_df['clusters'] = train_df['clusters'].replace({0: 1, 1: 3, 2: 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_df['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['vote_funny', 'vote_cool', 'vote_useful', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = train_df.review[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Good: -The absolute BEST deep dish pizza I have EVER had! -Great service (waiters/waitresses are fun and friendly) -Recommended by more Chicagoans I know than any other place when asked where to go for the BEST pizza The Bad: -None The Ugly: -Friend told me he had food poisoning from them once, to give you an idea of how good the pizza is, he still goes there regularly!\\n'"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.review[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_and_last_sentiment(txt, lexicon = lexicon):\n",
    "    s = txt.lower()\n",
    "    words = re.sub(r'[^\\w\\s]|(\\n)','',s).split(\" \")\n",
    "    words_df = pd.DataFrame(data = {\"word\": words})\n",
    "    words_df = words_df.merge(lexicon, how = \"left\", on = \"word\").fillna(0)\n",
    "    cutoff = round(len(words)/10)\n",
    "    first_sentiment = words_df[:cutoff].sentiment.sum()\n",
    "    last_sentiment = words_df[-cutoff:].sentiment.sum()\n",
    "    return first_sentiment, last_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_and_last_sentiment = list(map(get_first_and_last_sentiment, train_df.review.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentiment = list(map(lambda x: x[0], first_and_last_sentiment))\n",
    "last_sentiment = list(map(lambda x: x[1], first_and_last_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['first_sentiment'] = first_sentiment\n",
    "train_df['last_sentiment'] = last_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sentiment', 'first_sentiment', 'last_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['elipsis'] = train_df['elipsis'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(train_df[features]), train_df['rating'].ravel(), train_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = XGBClassifier(nrounds = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:31:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, nrounds=500, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-670-cb5888d55717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_boost_round\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "classif.train(num_boost_round = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt = train_df.review\n",
    "X_test_txt = test_df.review\n",
    "vectoriser = CountVectorizer()\n",
    "X_train = vectoriser.fit_transform(X_train_txt)\n",
    "y_train = np.array(train_df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28068x41952 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2394320 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84307089, 0.84467403, 0.8519772 , 0.84714057, 0.84429004])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942924326635314"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt = train_df.review\n",
    "X_test_txt = test_df.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adc = AdaBoostClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "nbc = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = CountVectorizer(ngram_range=(1, 1))\n",
    "X_train = vectoriser.fit_transform(X_train_txt)\n",
    "y_train = np.array(train_df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbc.fit(X_train, train_df.rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942924326635314"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbc.score(X_train, train_df.rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        dear longman & eagle.......you've left me no c...\n",
       "1        Delish. The hubby and I wanted to do brunch on...\n",
       "2        yep, I've giving Yolk 5 stars. It's just reall...\n",
       "3        Meat, meat, meat. It's meat-tastic. So much me...\n",
       "4        I caught up with the law school girls on a Sat...\n",
       "                               ...                        \n",
       "28063    This afternoon I went to Yolk with my college ...\n",
       "28064    Place has lots of side dishes. But that's abou...\n",
       "28065    I am a huge fan of Brazillian steakhouses I've...\n",
       "28066                                      Great Brunch.\\n\n",
       "28067    I love The Chicago Diner. I first came here ab...\n",
       "Name: review, Length: 28068, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22455x37960 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1918807 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = CountVectorizer(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffle(train_df[['rating', 'review']], random_state = 0)\n",
    "folds = np.array_split(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = np.array_split(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>5</td>\n",
       "      <td>Came in on a Saturday night before catching a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>5</td>\n",
       "      <td>Love your hot dogs Doug.Thanks for the great f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>3</td>\n",
       "      <td>Arg! I gotta get this out of the way right fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>5</td>\n",
       "      <td>Last year, I did a pho quest to a whole slew o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>5</td>\n",
       "      <td>This place is amazing--for BRUNCH. I've come o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16939</th>\n",
       "      <td>5</td>\n",
       "      <td>Me likey the macaroons and dah coffee. Goooood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>1</td>\n",
       "      <td>What a horrible place!! Going there was a very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>5</td>\n",
       "      <td>AMAAAAAAZING! I have never been to a fondue re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23051</th>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely loved this place!  **Warning - do n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>5</td>\n",
       "      <td>Did not really know that pizza could be so tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5614 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review\n",
       "11031       5  Came in on a Saturday night before catching a ...\n",
       "10977       5  Love your hot dogs Doug.Thanks for the great f...\n",
       "13996       3  Arg! I gotta get this out of the way right fro...\n",
       "3668        5  Last year, I did a pho quest to a whole slew o...\n",
       "6969        5  This place is amazing--for BRUNCH. I've come o...\n",
       "...       ...                                                ...\n",
       "16939       5  Me likey the macaroons and dah coffee. Goooood...\n",
       "28000       1  What a horrible place!! Going there was a very...\n",
       "573         5  AMAAAAAAZING! I have never been to a fondue re...\n",
       "23051       5  Absolutely loved this place!  **Warning - do n...\n",
       "17992       5  Did not really know that pizza could be so tho...\n",
       "\n",
       "[5614 rows x 2 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "train_fold = pd.concat(train_folds, axis = 0)\n",
    "test_fold = folds[fold_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold = pd.concat(train_folds, axis = 0)\n",
    "test_fold = folds[fold_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11031    Came in on a Saturday night before catching a ...\n",
       "10977    Love your hot dogs Doug.Thanks for the great f...\n",
       "13996    Arg! I gotta get this out of the way right fro...\n",
       "3668     Last year, I did a pho quest to a whole slew o...\n",
       "6969     This place is amazing--for BRUNCH. I've come o...\n",
       "                               ...                        \n",
       "13123    Ruth's Chris is an excellent steak house, in t...\n",
       "19648    I returned with my dad, and had an incredibly ...\n",
       "9845     Every time I visit Chicago, I try to visit the...\n",
       "10799    Bonsoiree is fabulous! We had the 8 course men...\n",
       "2732     Third time here, although this place is good, ...\n",
       "Name: review, Length: 22455, dtype: object"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fold.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = CountVectorizer()\n",
    "vectoriser.fit(X_train_txt)\n",
    "X_train = vectoriser.transform(X_train_txt)\n",
    "X_test = vectoriser.transform(X_test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"multinomial_naive_bayes\": MultinomialNB(),\n",
    "    \"random_forest_classifier\": RandomForestClassifier(),\n",
    "    \"adaboost_classifier\": AdaBoostClassifier(),\n",
    "    \"logistic_regression\": LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            multinomial_naive_bayes took 0.01 seconds to train\n",
      "            and has training accuracy 0.897 and testing accuracy 0.849\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            multinomial_naive_bayes took 0.01 seconds to train\n",
      "            and has training accuracy 0.899 and testing accuracy 0.844\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            multinomial_naive_bayes took 0.01 seconds to train\n",
      "            and has training accuracy 0.899 and testing accuracy 0.852\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            multinomial_naive_bayes took 0.01 seconds to train\n",
      "            and has training accuracy 0.900 and testing accuracy 0.852\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            multinomial_naive_bayes took 0.02 seconds to train\n",
      "            and has training accuracy 0.899 and testing accuracy 0.844\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for multinomial_naive_bayes was 0.848\n",
      "\n",
      "            random_forest_classifier took 32.91 seconds to train\n",
      "            and has training accuracy 1.000 and testing accuracy 0.728\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            random_forest_classifier took 33.53 seconds to train\n",
      "            and has training accuracy 1.000 and testing accuracy 0.728\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            random_forest_classifier took 32.49 seconds to train\n",
      "            and has training accuracy 1.000 and testing accuracy 0.735\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            random_forest_classifier took 31.75 seconds to train\n",
      "            and has training accuracy 1.000 and testing accuracy 0.730\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            random_forest_classifier took 31.81 seconds to train\n",
      "            and has training accuracy 1.000 and testing accuracy 0.729\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for random_forest_classifier was 0.730\n",
      "\n",
      "            adaboost_classifier took 4.68 seconds to train\n",
      "            and has training accuracy 0.795 and testing accuracy 0.796\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            adaboost_classifier took 4.66 seconds to train\n",
      "            and has training accuracy 0.797 and testing accuracy 0.796\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            adaboost_classifier took 4.66 seconds to train\n",
      "            and has training accuracy 0.797 and testing accuracy 0.797\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            adaboost_classifier took 4.65 seconds to train\n",
      "            and has training accuracy 0.795 and testing accuracy 0.795\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            adaboost_classifier took 4.65 seconds to train\n",
      "            and has training accuracy 0.798 and testing accuracy 0.792\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for adaboost_classifier was 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            logistic_regression took 3.00 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.861\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            logistic_regression took 2.93 seconds to train\n",
      "            and has training accuracy 0.948 and testing accuracy 0.858\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            logistic_regression took 2.82 seconds to train\n",
      "            and has training accuracy 0.949 and testing accuracy 0.857\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            logistic_regression took 2.93 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.855\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            logistic_regression took 2.98 seconds to train\n",
      "            and has training accuracy 0.954 and testing accuracy 0.859\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for logistic_regression was 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "for classifier in list(classifiers.keys()):\n",
    "    \n",
    "    clf = classifiers[classifier]\n",
    "    \n",
    "    test_scores = []\n",
    "    \n",
    "    for fold_n in range(5):\n",
    "\n",
    "        train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "        train_fold = pd.concat(train_folds, axis = 0)\n",
    "        test_fold = folds[fold_n]\n",
    "\n",
    "        X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "        X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])\n",
    "\n",
    "        vectoriser.fit(X_train_txt)\n",
    "        X_train = vectoriser.transform(X_train_txt)\n",
    "        X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "        before = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        after = time.time()\n",
    "        train_time = after - before\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "        print(\n",
    "            '''\n",
    "            {} took {:.2f} seconds to train\n",
    "            and has training accuracy {:.3f} and testing accuracy {:.3f}\n",
    "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "            '''.format(classifier, train_time, train_score, test_score)\n",
    "        )\n",
    "    \n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    \n",
    "    print(\"the mean test score for {} was {:.3f}\".format(classifier, mean_test_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like both the random forest classifier and logistic regression are severely overfitting. Lets play around with some different parameters to see if we can get the models to do a bit better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=1000, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10, 100, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_values = {'C': [0.01,0.1,1,10,100,1000]}\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(clf, grid_values)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.640097</td>\n",
       "      <td>0.080018</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>3.742488e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>0.849922</td>\n",
       "      <td>0.853039</td>\n",
       "      <td>0.845469</td>\n",
       "      <td>0.843242</td>\n",
       "      <td>0.848586</td>\n",
       "      <td>0.848052</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.917177</td>\n",
       "      <td>0.897295</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.784161e-07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.861501</td>\n",
       "      <td>0.863059</td>\n",
       "      <td>0.855934</td>\n",
       "      <td>0.855711</td>\n",
       "      <td>0.861501</td>\n",
       "      <td>0.859541</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.865662</td>\n",
       "      <td>0.889156</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.784161e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.855043</td>\n",
       "      <td>0.860165</td>\n",
       "      <td>0.847473</td>\n",
       "      <td>0.852149</td>\n",
       "      <td>0.856602</td>\n",
       "      <td>0.854286</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.019686</td>\n",
       "      <td>0.285765</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>7.629395e-07</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.843687</td>\n",
       "      <td>0.854153</td>\n",
       "      <td>0.843465</td>\n",
       "      <td>0.844578</td>\n",
       "      <td>0.840125</td>\n",
       "      <td>0.845202</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.858453</td>\n",
       "      <td>0.154306</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>3.958702e-04</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.832331</td>\n",
       "      <td>0.842797</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.828991</td>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.833088</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.573138</td>\n",
       "      <td>0.088159</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.784161e-07</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "      <td>0.824093</td>\n",
       "      <td>0.835894</td>\n",
       "      <td>0.822089</td>\n",
       "      <td>0.820753</td>\n",
       "      <td>0.828991</td>\n",
       "      <td>0.826364</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       4.640097      0.080018         0.003002    3.742488e-06    0.01   \n",
       "1       9.917177      0.897295         0.003000    1.784161e-07     0.1   \n",
       "2      18.865662      0.889156         0.003000    1.784161e-07       1   \n",
       "3      25.019686      0.285765         0.003000    7.629395e-07      10   \n",
       "4      24.858453      0.154306         0.003198    3.958702e-04     100   \n",
       "5      24.573138      0.088159         0.003000    1.784161e-07    1000   \n",
       "\n",
       "        params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.01}           0.849922           0.853039           0.845469   \n",
       "1   {'C': 0.1}           0.861501           0.863059           0.855934   \n",
       "2     {'C': 1}           0.855043           0.860165           0.847473   \n",
       "3    {'C': 10}           0.843687           0.854153           0.843465   \n",
       "4   {'C': 100}           0.832331           0.842797           0.831663   \n",
       "5  {'C': 1000}           0.824093           0.835894           0.822089   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.843242           0.848586         0.848052        0.003420   \n",
       "1           0.855711           0.861501         0.859541        0.003090   \n",
       "2           0.852149           0.856602         0.854286        0.004278   \n",
       "3           0.844578           0.840125         0.845202        0.004724   \n",
       "4           0.828991           0.829659         0.833088        0.005008   \n",
       "5           0.820753           0.828991         0.826364        0.005525   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                1  \n",
       "2                2  \n",
       "3                4  \n",
       "4                5  \n",
       "5                6  "
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like for logistic regression 0.1 is the best regularization parameter... this doenst really do that much better than the default parameter of 1 though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [10, 50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [10, 50, 100],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.094977</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.111506</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'auto'}</td>\n",
       "      <td>0.688265</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688087</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.129536</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.111973</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.688265</td>\n",
       "      <td>0.688265</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688132</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.106161</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'log2'}</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.318649</td>\n",
       "      <td>0.261314</td>\n",
       "      <td>0.168525</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>50</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'auto'}</td>\n",
       "      <td>0.716099</td>\n",
       "      <td>0.719662</td>\n",
       "      <td>0.712314</td>\n",
       "      <td>0.713204</td>\n",
       "      <td>0.716322</td>\n",
       "      <td>0.715520</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.145186</td>\n",
       "      <td>0.259849</td>\n",
       "      <td>0.164552</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>50</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.716990</td>\n",
       "      <td>0.714540</td>\n",
       "      <td>0.716544</td>\n",
       "      <td>0.717435</td>\n",
       "      <td>0.712982</td>\n",
       "      <td>0.715698</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.750251</td>\n",
       "      <td>0.088108</td>\n",
       "      <td>0.153628</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>50</td>\n",
       "      <td>log2</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2'}</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.688043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.398458</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.191634</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 100, 'max_features': 'auto'}</td>\n",
       "      <td>0.729014</td>\n",
       "      <td>0.725674</td>\n",
       "      <td>0.723670</td>\n",
       "      <td>0.720330</td>\n",
       "      <td>0.725006</td>\n",
       "      <td>0.724738</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.436250</td>\n",
       "      <td>0.209925</td>\n",
       "      <td>0.189154</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>100</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 100, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.727678</td>\n",
       "      <td>0.725228</td>\n",
       "      <td>0.725896</td>\n",
       "      <td>0.725896</td>\n",
       "      <td>0.726787</td>\n",
       "      <td>0.726297</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.086935</td>\n",
       "      <td>0.052781</td>\n",
       "      <td>0.200188</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>100</td>\n",
       "      <td>log2</td>\n",
       "      <td>{'max_depth': 100, 'max_features': 'log2'}</td>\n",
       "      <td>0.688711</td>\n",
       "      <td>0.689156</td>\n",
       "      <td>0.688488</td>\n",
       "      <td>0.688933</td>\n",
       "      <td>0.689156</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       3.094977      0.018735         0.111506        0.002153   \n",
       "1       3.129536      0.038128         0.111973        0.001764   \n",
       "2       0.921014      0.025196         0.106161        0.000823   \n",
       "3      20.318649      0.261314         0.168525        0.008842   \n",
       "4      20.145186      0.259849         0.164552        0.001706   \n",
       "5       4.750251      0.088108         0.153628        0.001275   \n",
       "6      30.398458      0.156999         0.191634        0.007207   \n",
       "7      30.436250      0.209925         0.189154        0.002122   \n",
       "8      10.086935      0.052781         0.200188        0.002294   \n",
       "\n",
       "  param_max_depth param_max_features  \\\n",
       "0              10               auto   \n",
       "1              10               sqrt   \n",
       "2              10               log2   \n",
       "3              50               auto   \n",
       "4              50               sqrt   \n",
       "5              50               log2   \n",
       "6             100               auto   \n",
       "7             100               sqrt   \n",
       "8             100               log2   \n",
       "\n",
       "                                       params  split0_test_score  \\\n",
       "0   {'max_depth': 10, 'max_features': 'auto'}           0.688265   \n",
       "1   {'max_depth': 10, 'max_features': 'sqrt'}           0.688265   \n",
       "2   {'max_depth': 10, 'max_features': 'log2'}           0.688043   \n",
       "3   {'max_depth': 50, 'max_features': 'auto'}           0.716099   \n",
       "4   {'max_depth': 50, 'max_features': 'sqrt'}           0.716990   \n",
       "5   {'max_depth': 50, 'max_features': 'log2'}           0.688043   \n",
       "6  {'max_depth': 100, 'max_features': 'auto'}           0.729014   \n",
       "7  {'max_depth': 100, 'max_features': 'sqrt'}           0.727678   \n",
       "8  {'max_depth': 100, 'max_features': 'log2'}           0.688711   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.688043           0.688043           0.688043           0.688043   \n",
       "1           0.688265           0.688043           0.688043           0.688043   \n",
       "2           0.688043           0.688043           0.688043           0.688043   \n",
       "3           0.719662           0.712314           0.713204           0.716322   \n",
       "4           0.714540           0.716544           0.717435           0.712982   \n",
       "5           0.688043           0.688043           0.688043           0.688043   \n",
       "6           0.725674           0.723670           0.720330           0.725006   \n",
       "7           0.725228           0.725896           0.725896           0.726787   \n",
       "8           0.689156           0.688488           0.688933           0.689156   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.688087        0.000089                7  \n",
       "1         0.688132        0.000109                6  \n",
       "2         0.688043        0.000000                8  \n",
       "3         0.715520        0.002599                4  \n",
       "4         0.715698        0.001681                3  \n",
       "5         0.688043        0.000000                8  \n",
       "6         0.724738        0.002821                2  \n",
       "7         0.726297        0.000850                1  \n",
       "8         0.688889        0.000260                5  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like parameter tuning for randomforest also doesnt make a huge difference. Also bear in mind tree based algorithms are prone to overfit the training data. it's not a huge problem if i cant eek out better performance. is this a question of the grid representation???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool, lets try stacking models and seeing if they perform any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"multinomial_naive_bayes\": MultinomialNB(),\n",
    "    \"random_forest_classifier\": RandomForestClassifier(max_depth = 50),\n",
    "    \"adaboost_classifier\": AdaBoostClassifier(),\n",
    "    \"logistic_regression\": LogisticRegression(C = 0.01)\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "    for fold_n in range(5):\n",
    "\n",
    "        train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "        train_fold = pd.concat(train_folds, axis = 0)\n",
    "        test_fold = folds[fold_n]\n",
    "\n",
    "        X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "        X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])\n",
    "\n",
    "        vectoriser.fit(X_train_txt)\n",
    "        X_train = vectoriser.transform(X_train_txt)\n",
    "        X_test = vectoriser.transform(X_test_txt)\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        predictions_list = []\n",
    "        \n",
    "        for classifier in list(classifiers.keys()):\n",
    "            \n",
    "            clf = classifiers[classifier]\n",
    "        \n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            predictions = clf.predict(X_test)\n",
    "            \n",
    "            predictions_list.append(predictions)\n",
    "        \n",
    "        after = time.time()\n",
    "        \n",
    "        train_time = after - before\n",
    "        \n",
    "        meta_classifier = LogisticRegression()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_score = clf.score(X_train, y_train)\n",
    "            test_score = clf.score(X_test, y_test)\n",
    "\n",
    "            test_scores.append(test_score)\n",
    "        \n",
    "        print(train_time)\n",
    "\n",
    "            print(\n",
    "                '''\n",
    "                {} took {:.2f} seconds to train\n",
    "                and has training accuracy {:.3f} and testing accuracy {:.3f}\n",
    "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "                '''.format(classifier, train_time, train_score, test_score)\n",
    "            )\n",
    "    \n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    \n",
    "    print(\"the mean test score for {} was {:.3f}\".format(classifier, mean_test_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectoriser.fit(X_train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>vote_funny</th>\n",
       "      <th>vote_cool</th>\n",
       "      <th>vote_useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is 10am on a Monday morning and my wife say...</td>\n",
       "      <td>10/17/2011</td>\n",
       "      <td>WwAdD9wjkLqZHOO5</td>\n",
       "      <td>CY9iLsE2z_yLhLqJdD1WGw</td>\n",
       "      <td>cQnY_VneZisfUAqcbuEuKg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I came here with a friend for her work thing -...</td>\n",
       "      <td>3/19/2007</td>\n",
       "      <td>Rd-Vra4drjDI8AQCXf6yTA</td>\n",
       "      <td>6feRQ3I9RpxRlEX7gPuJRg</td>\n",
       "      <td>sfWMOqUEp8S2adDeJp7Kzg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATTENTION!!! DO NOT GO TO THIS RESTAURANT EVER...</td>\n",
       "      <td>12/27/2008</td>\n",
       "      <td>mJD</td>\n",
       "      <td>RQs0smGxdXIlBqqlNP7pNg</td>\n",
       "      <td>2WoMT3wSpp9vxZeTv6u-cw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree, with Jonathan S. - this place is a 3....</td>\n",
       "      <td>6/1/2010</td>\n",
       "      <td>J-xkladMPiVZFVhIbXLNEQ</td>\n",
       "      <td>q6EF83uL2lFRtnWwrfaYGA</td>\n",
       "      <td>AqgG-1aD6JYj9D6OmBWO3w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First visit to Chicago, and a friend recommend...</td>\n",
       "      <td>9/4/2012</td>\n",
       "      <td>jOMycoLCy79-256vNxl1kA</td>\n",
       "      <td>erZZ9K9wgdSswAm6TEh7Qw</td>\n",
       "      <td>boE4Ahsssqic7o5wQLI04w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>I LOVED XOCO! It was amazing! My friends and I...</td>\n",
       "      <td>1/22/2010</td>\n",
       "      <td>QcCptbqQUgYwqeKz6Roncg</td>\n",
       "      <td>wISjGJS3bb6dh1rs2f1c1Q</td>\n",
       "      <td>jGiKIJCVLZHXQDSNnSLPsw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>Place is pretty dark. A place to chat with a g...</td>\n",
       "      <td>6/14/2012</td>\n",
       "      <td>LG3FrYu03BCqQ-TAGsbowA</td>\n",
       "      <td>0eoCRbkKsEj8iv2Aow2ong</td>\n",
       "      <td>oEFJ29zAQaCNnQzebHQvpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>Get to Xoco and you can order yourself a tasty...</td>\n",
       "      <td>5/10/2010</td>\n",
       "      <td>Y2IKY1Nbmt2rjbiVyJoSSg</td>\n",
       "      <td>eiMZGw1zY8xwk1Xc3GM8lw</td>\n",
       "      <td>jGiKIJCVLZHXQDSNnSLPsw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>I'm writing this less than 10 minutes after re...</td>\n",
       "      <td>6/9/2011</td>\n",
       "      <td>OrKMrbD8J2J-0jLPip6ifA</td>\n",
       "      <td>L80nu6pXxl07zb_IKn0ZLw</td>\n",
       "      <td>UQ3cGi3GdBljE3i2_qLcBQ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>I love how this gem is hidden. I felt like Ali...</td>\n",
       "      <td>12/8/2009</td>\n",
       "      <td>GAbxiT96Hs-wcvl2HT4AVw</td>\n",
       "      <td>cmrrkHcGmFcy68k372Jp0w</td>\n",
       "      <td>oEFJ29zAQaCNnQzebHQvpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review        date  \\\n",
       "0     It is 10am on a Monday morning and my wife say...  10/17/2011   \n",
       "1     I came here with a friend for her work thing -...   3/19/2007   \n",
       "2     ATTENTION!!! DO NOT GO TO THIS RESTAURANT EVER...  12/27/2008   \n",
       "3     I agree, with Jonathan S. - this place is a 3....    6/1/2010   \n",
       "4     First visit to Chicago, and a friend recommend...    9/4/2012   \n",
       "...                                                 ...         ...   \n",
       "7013  I LOVED XOCO! It was amazing! My friends and I...   1/22/2010   \n",
       "7014  Place is pretty dark. A place to chat with a g...   6/14/2012   \n",
       "7015  Get to Xoco and you can order yourself a tasty...   5/10/2010   \n",
       "7016  I'm writing this less than 10 minutes after re...    6/9/2011   \n",
       "7017  I love how this gem is hidden. I felt like Ali...   12/8/2009   \n",
       "\n",
       "                   review_id             reviewer_id             business_id  \\\n",
       "0           WwAdD9wjkLqZHOO5  CY9iLsE2z_yLhLqJdD1WGw  cQnY_VneZisfUAqcbuEuKg   \n",
       "1     Rd-Vra4drjDI8AQCXf6yTA  6feRQ3I9RpxRlEX7gPuJRg  sfWMOqUEp8S2adDeJp7Kzg   \n",
       "2                        mJD  RQs0smGxdXIlBqqlNP7pNg  2WoMT3wSpp9vxZeTv6u-cw   \n",
       "3     J-xkladMPiVZFVhIbXLNEQ  q6EF83uL2lFRtnWwrfaYGA  AqgG-1aD6JYj9D6OmBWO3w   \n",
       "4     jOMycoLCy79-256vNxl1kA  erZZ9K9wgdSswAm6TEh7Qw  boE4Ahsssqic7o5wQLI04w   \n",
       "...                      ...                     ...                     ...   \n",
       "7013  QcCptbqQUgYwqeKz6Roncg  wISjGJS3bb6dh1rs2f1c1Q  jGiKIJCVLZHXQDSNnSLPsw   \n",
       "7014  LG3FrYu03BCqQ-TAGsbowA  0eoCRbkKsEj8iv2Aow2ong  oEFJ29zAQaCNnQzebHQvpg   \n",
       "7015  Y2IKY1Nbmt2rjbiVyJoSSg  eiMZGw1zY8xwk1Xc3GM8lw  jGiKIJCVLZHXQDSNnSLPsw   \n",
       "7016  OrKMrbD8J2J-0jLPip6ifA  L80nu6pXxl07zb_IKn0ZLw  UQ3cGi3GdBljE3i2_qLcBQ   \n",
       "7017  GAbxiT96Hs-wcvl2HT4AVw  cmrrkHcGmFcy68k372Jp0w  oEFJ29zAQaCNnQzebHQvpg   \n",
       "\n",
       "      vote_funny  vote_cool  vote_useful  \n",
       "0              2          1            3  \n",
       "1              0          1            3  \n",
       "2              0          0            0  \n",
       "3              0          0            0  \n",
       "4              0          0            0  \n",
       "...          ...        ...          ...  \n",
       "7013           0          0            0  \n",
       "7014           2          0            0  \n",
       "7015           0          0            1  \n",
       "7016           3          0            4  \n",
       "7017           0          3            3  \n",
       "\n",
       "[7018 rows x 8 columns]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train, meta_test = train_test_split(train_df[['rating', 'review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12288</th>\n",
       "      <td>5</td>\n",
       "      <td>Great beer selection. Tasty moules. What else ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>5</td>\n",
       "      <td>I love love love Pequods! Great pan pizza in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>5</td>\n",
       "      <td>Where else can you eat veal heart while the Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20097</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm somewhat of a gastropub junkie, and as suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20763</th>\n",
       "      <td>5</td>\n",
       "      <td>Alinea, oh yes. I would go here every week if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25628</th>\n",
       "      <td>5</td>\n",
       "      <td>I give 5 star reviews to places I would eat at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13519</th>\n",
       "      <td>5</td>\n",
       "      <td>i say goddamn! when it comes to sushi rolls, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>3</td>\n",
       "      <td>There is no doubt about it, this place has goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27958</th>\n",
       "      <td>5</td>\n",
       "      <td>Great place. I recently heard of this place an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18998</th>\n",
       "      <td>5</td>\n",
       "      <td>One of the best places to grab food at if you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7017 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review\n",
       "12288       5  Great beer selection. Tasty moules. What else ...\n",
       "4665        5  I love love love Pequods! Great pan pizza in f...\n",
       "6170        5  Where else can you eat veal heart while the Co...\n",
       "20097       5  I'm somewhat of a gastropub junkie, and as suc...\n",
       "20763       5  Alinea, oh yes. I would go here every week if ...\n",
       "...       ...                                                ...\n",
       "25628       5  I give 5 star reviews to places I would eat at...\n",
       "13519       5  i say goddamn! when it comes to sushi rolls, i...\n",
       "2396        3  There is no doubt about it, this place has goo...\n",
       "27958       5  Great place. I recently heard of this place an...\n",
       "18998       5  One of the best places to grab food at if you ...\n",
       "\n",
       "[7017 rows x 2 columns]"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = list(map(get_sentiment, X_test_txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier(nrounds = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "multinomial_naive_bayes accuracy 0.7513654713844693\n",
      "random_forest_classifier accuracy 0.7048207076703871\n",
      "adaboost_classifier accuracy 0.8000474946568511\n",
      "logistic_regression accuracy 0.8658275943956305\n",
      "[15:45:56] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost accuracy 0.8639278081215863\n",
      "83.33362364768982\n",
      "stacking accuracy is 0.8693173720963374\n",
      "1\n",
      "multinomial_naive_bayes accuracy 0.7396674584323041\n",
      "random_forest_classifier accuracy 0.694061757719715\n",
      "adaboost_classifier accuracy 0.7971496437054632\n",
      "logistic_regression accuracy 0.856769596199525\n",
      "[15:47:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost accuracy 0.8522565320665083\n",
      "83.72721910476685\n",
      "stacking accuracy is 0.8720250819438506\n",
      "2\n",
      "multinomial_naive_bayes accuracy 0.7391923990498812\n",
      "random_forest_classifier accuracy 0.6912114014251781\n",
      "adaboost_classifier accuracy 0.7781472684085511\n",
      "logistic_regression accuracy 0.8532066508313539\n",
      "[15:49:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost accuracy 0.8513064133016627\n",
      "82.66828298568726\n",
      "stacking accuracy is 0.8673222174718541\n",
      "3\n",
      "multinomial_naive_bayes accuracy 0.7451306413301663\n",
      "random_forest_classifier accuracy 0.6938242280285035\n",
      "adaboost_classifier accuracy 0.7935866983372921\n",
      "logistic_regression accuracy 0.8551068883610451\n",
      "[15:50:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost accuracy 0.8546318289786223\n",
      "83.0404269695282\n",
      "stacking accuracy is 0.8703149494085791\n",
      "4\n",
      "multinomial_naive_bayes accuracy 0.7401425178147268\n",
      "random_forest_classifier accuracy 0.695249406175772\n",
      "adaboost_classifier accuracy 0.7914489311163896\n",
      "logistic_regression accuracy 0.8501187648456057\n",
      "[15:52:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost accuracy 0.859144893111639\n",
      "81.83563756942749\n",
      "stacking accuracy is 0.8764429243266353\n"
     ]
    }
   ],
   "source": [
    "vectoriser = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "scores = []\n",
    "\n",
    "classifiers = {\n",
    "    \"multinomial_naive_bayes\": MultinomialNB(),\n",
    "    \"random_forest_classifier\": RandomForestClassifier(max_depth = 50),\n",
    "    \"adaboost_classifier\": AdaBoostClassifier(),\n",
    "    \"logistic_regression\": LogisticRegression(C = 0.01, max_iter=1000),\n",
    "    \"xgboost\": XGBClassifier(nrounds = 500)\n",
    "}\n",
    "\n",
    "# loop\n",
    "\n",
    "\n",
    "for fold_n in range(5):\n",
    "    \n",
    "    print(fold_n)\n",
    "    \n",
    "    meta_train, meta_test = train_test_split(train_df[['rating', 'review']])\n",
    "    \n",
    "    train = shuffle(meta_train)\n",
    "    \n",
    "    folds = np.array_split(train, 5)\n",
    "\n",
    "    train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "    train_fold = pd.concat(train_folds, axis = 0)\n",
    "    test_fold = folds[fold_n]\n",
    "\n",
    "    X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "    X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])\n",
    "\n",
    "    vectoriser.fit(X_train_txt)\n",
    "    X_train = vectoriser.transform(X_train_txt)\n",
    "    X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "    before = time.time()\n",
    "    \n",
    "    clfs = []\n",
    "    \n",
    "    predictions_list = []\n",
    "    \n",
    "    for classifier in list(classifiers.keys()):\n",
    "\n",
    "        clf = classifiers[classifier]\n",
    "\n",
    "        clfs.append(clf)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        print(\"{} accuracy {}\".format(classifier, score))\n",
    "\n",
    "        predictions = clf.predict(X_test)\n",
    "\n",
    "        predictions_list.append(predictions)\n",
    "\n",
    "    after = time.time()\n",
    "\n",
    "    print(after - before)\n",
    "\n",
    "    meta_clf = RandomForestClassifier()\n",
    "\n",
    "    meta_clf.fit(np.stack(predictions_list).transpose(), y_test)\n",
    "\n",
    "    # lets test the meta classifier on the meta test data\n",
    "\n",
    "    reviews = vectoriser.transform(meta_test.review)\n",
    "\n",
    "    new_predictions = []\n",
    "\n",
    "    for clf in clfs:\n",
    "        predictions = clf.predict(reviews)\n",
    "        new_predictions.append(predictions)\n",
    "\n",
    "    score = meta_clf.score(np.stack(new_predictions).transpose(), meta_test.rating)\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "    print(\"stacking accuracy is {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performs worse when adding some sort of sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8710845090494514"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interestingly the performance of each individual classifier gets worse, but when i stack them the performance is better!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performs slightly better than the individual classifiers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets train one on the whole set and then try it on Kaggle!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:55:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = {\n",
    "    \"multinomial_naive_bayes\": MultinomialNB(),\n",
    "    \"random_forest_classifier\": RandomForestClassifier(max_depth = 50),\n",
    "    \"adaboost_classifier\": AdaBoostClassifier(),\n",
    "    \"logistic_regression\": LogisticRegression(C = 0.01, max_iter = 1000),\n",
    "    \"xgboost\": XGBClassifier(nrounds = 500)\n",
    "}\n",
    "\n",
    "# loop\n",
    "    \n",
    "meta_train, meta_test = train_test_split(train_df[['rating', 'review']])\n",
    "\n",
    "train = shuffle(meta_train)\n",
    "\n",
    "vectoriser.fit(meta_train.review)\n",
    "X_train = vectoriser.transform(meta_train.review)\n",
    "y_train = meta_train.rating\n",
    "\n",
    "X_test = vectoriser.transform(meta_test.review)\n",
    "y_test = meta_test.rating\n",
    "\n",
    "clfs = []\n",
    "\n",
    "predictions_list = []\n",
    "    \n",
    "for classifier in list(classifiers.keys()):\n",
    "\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    clfs.append(clf)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "meta_clf = RandomForestClassifier()\n",
    "\n",
    "meta_clf.fit(np.stack(predictions_list).transpose(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectoriser.transform(test_df.review)\n",
    "\n",
    "predictions_list = []\n",
    "    \n",
    "for classifier in list(classifiers.keys()):\n",
    "\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    predictions_list.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = meta_clf.predict(np.stack(predictions_list).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(list(enumerate(predictions, start = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.rename(columns = {0: 'instance_id', 1: 'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"predictions_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>7014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>7015</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>7016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>7017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>7018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id  rating\n",
       "0               1       5\n",
       "1               2       5\n",
       "2               3       1\n",
       "3               4       3\n",
       "4               5       5\n",
       "...           ...     ...\n",
       "7013         7014       5\n",
       "7014         7015       5\n",
       "7015         7016       5\n",
       "7016         7017       5\n",
       "7017         7018       5\n",
       "\n",
       "[7018 rows x 2 columns]"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(start = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
