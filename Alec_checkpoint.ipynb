{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Natural language toolkit. Download if not installed already\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# For splitting by punctuation and using regex\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Useful\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Potentially used Models\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Evaluation and feature selection tools\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is 10am on a Monday morning and my wife say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I came here with a friend for her work thing -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATTENTION!!! DO NOT GO TO THIS RESTAURANT EVER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree, with Jonathan S. - this place is a 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First visit to Chicago, and a friend recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was so impressed with this place! The servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fantastic little place! The food was so CHEAP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>One of the best dogs I've ever had. EVER! To c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>As a New Yorker visiting Chicago I went to The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>had the pleasure of going back to the gage thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Just went here for the fifth time and this pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I live in NYC and thought I'd check out how Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I am admittedly a pretty huge pizza snob. Pizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Authentic Costa Rican food. Lots of long revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Was this place worth traveling to Chicago for?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This place was so bad I had to put ketchup on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I've been to a few different Brazilian steak h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The tap handles are super cool...almost as goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Food is fantastic! Atmosphere is chill. Servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I did the call in/pick up and knew from the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>One of the best omelets I ever had, and the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Stay Away. Never have I experienced worse serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>By far the most amazing mexican restaurant in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Came for the chicken wings only because I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>THE CAST OF CHARACTERS: One father, one mother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LOVE this place. I always recommend this place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hour long wait outside. Cool, air-conditioned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ordered delivery. I had the seoul sassy and a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The pizzas we got were AMAZING! Best pizza I'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>To preface this, it probably it not really 1 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>Some of the previous reviewers need to relax. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>Took my woman on a long overdue date here last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>Perfect spot for a brunch then walk in Grant P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>Food was just okay. Good selection for lunch b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>If there was a dislike button on this review, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>One of my friends always loved this place and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>It's one of my regular stops every time I head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>My favorite japanese/ Asian restaurant! The su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>Great Cuban Sandwich!!! 10 dollar lunch worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>Best steak and vegetarian tacos in town. Any m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>Brought a large party there for the hubby's bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>Not a bad place but it didnt really stand out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>I frickin love this place. I went by it on a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>We were very torn between coming here or going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>eh.... not very impressive. Too ordinary for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>I have been here more times than I can remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>Second time around everything was not that gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>fan.for.life. they cancelled our reservation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>Ha Ha!!!! Finally The AW has made it to Dougs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>Seriously outstanding. Last week, my colleague...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>Biggest disappointment in a long time. Schizo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>MEAT GLORIOUS MEAT. I felt like I was going to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>My boyfriend and I enjoyed an incredibly delic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>Completely over-rated. Pizza came out warn and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>For a $100+ per person meal I was expecting mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>I LOVED XOCO! It was amazing! My friends and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>Place is pretty dark. A place to chat with a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>Get to Xoco and you can order yourself a tasty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>I'm writing this less than 10 minutes after re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>I love how this gem is hidden. I felt like Ali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review\n",
       "0     It is 10am on a Monday morning and my wife say...\n",
       "1     I came here with a friend for her work thing -...\n",
       "2     ATTENTION!!! DO NOT GO TO THIS RESTAURANT EVER...\n",
       "3     I agree, with Jonathan S. - this place is a 3....\n",
       "4     First visit to Chicago, and a friend recommend...\n",
       "5     I was so impressed with this place! The servic...\n",
       "6     Fantastic little place! The food was so CHEAP,...\n",
       "7     One of the best dogs I've ever had. EVER! To c...\n",
       "8     As a New Yorker visiting Chicago I went to The...\n",
       "9     had the pleasure of going back to the gage thi...\n",
       "10    Just went here for the fifth time and this pla...\n",
       "11    I live in NYC and thought I'd check out how Ch...\n",
       "12    I am admittedly a pretty huge pizza snob. Pizz...\n",
       "13    Authentic Costa Rican food. Lots of long revie...\n",
       "14    Was this place worth traveling to Chicago for?...\n",
       "15    This place was so bad I had to put ketchup on ...\n",
       "16    I've been to a few different Brazilian steak h...\n",
       "17    The tap handles are super cool...almost as goo...\n",
       "18    Food is fantastic! Atmosphere is chill. Servic...\n",
       "19    I did the call in/pick up and knew from the mo...\n",
       "20    One of the best omelets I ever had, and the po...\n",
       "21    Stay Away. Never have I experienced worse serv...\n",
       "22    By far the most amazing mexican restaurant in ...\n",
       "23    Came for the chicken wings only because I was ...\n",
       "24    THE CAST OF CHARACTERS: One father, one mother...\n",
       "25    LOVE this place. I always recommend this place...\n",
       "26    Hour long wait outside. Cool, air-conditioned ...\n",
       "27    Ordered delivery. I had the seoul sassy and a ...\n",
       "28    The pizzas we got were AMAZING! Best pizza I'v...\n",
       "29    To preface this, it probably it not really 1 s...\n",
       "...                                                 ...\n",
       "6988  Some of the previous reviewers need to relax. ...\n",
       "6989  Took my woman on a long overdue date here last...\n",
       "6990  Perfect spot for a brunch then walk in Grant P...\n",
       "6991  Food was just okay. Good selection for lunch b...\n",
       "6992  If there was a dislike button on this review, ...\n",
       "6993  One of my friends always loved this place and ...\n",
       "6994  It's one of my regular stops every time I head...\n",
       "6995  My favorite japanese/ Asian restaurant! The su...\n",
       "6996  Great Cuban Sandwich!!! 10 dollar lunch worth ...\n",
       "6997  Best steak and vegetarian tacos in town. Any m...\n",
       "6998  Brought a large party there for the hubby's bi...\n",
       "6999  Not a bad place but it didnt really stand out ...\n",
       "7000  I frickin love this place. I went by it on a v...\n",
       "7001  We were very torn between coming here or going...\n",
       "7002  eh.... not very impressive. Too ordinary for a...\n",
       "7003  I have been here more times than I can remembe...\n",
       "7004  Second time around everything was not that gre...\n",
       "7005  fan.for.life. they cancelled our reservation o...\n",
       "7006  Ha Ha!!!! Finally The AW has made it to Dougs ...\n",
       "7007  Seriously outstanding. Last week, my colleague...\n",
       "7008  Biggest disappointment in a long time. Schizo ...\n",
       "7009  MEAT GLORIOUS MEAT. I felt like I was going to...\n",
       "7010  My boyfriend and I enjoyed an incredibly delic...\n",
       "7011  Completely over-rated. Pizza came out warn and...\n",
       "7012  For a $100+ per person meal I was expecting mo...\n",
       "7013  I LOVED XOCO! It was amazing! My friends and I...\n",
       "7014  Place is pretty dark. A place to chat with a g...\n",
       "7015  Get to Xoco and you can order yourself a tasty...\n",
       "7016  I'm writing this less than 10 minutes after re...\n",
       "7017  I love how this gem is hidden. I felt like Ali...\n",
       "\n",
       "[7018 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test not useful right now\n",
    "test_meta = pd.read_csv(\"review_meta_test.csv\", sep=\",\")\n",
    "test_text = pd.read_csv(\"review_text_test.csv\", sep=',')\n",
    "\n",
    "# Also not useful right now\n",
    "train_meta = pd.read_csv(\"review_meta_train.csv\", sep=\",\")\n",
    "train_text = pd.read_csv(\"review_text_train.csv\", sep=\",\")\n",
    "\n",
    "# Ids are not predictive. Consider adding in the date?\n",
    "train_meta = train_meta.drop([\"date\", \"review_id\", \"reviewer_id\", \"business_id\"], axis = 1)\n",
    "\n",
    "# Combine the meta features and text. MoveA class to end\n",
    "train_data = pd.concat([train_meta, train_text], axis=1)\n",
    "class_col = train_data.pop(\"rating\")\n",
    "train_data[\"rating\"] = class_col\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28068, 41648)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems like manually using countvectoriser() is better than sparse matrix\n",
    "\n",
    "# countvec\n",
    "import pickle\n",
    "vocab = pickle.load(open(\"review_text_features_countvec/train_countvectorizer.pkl\", \"rb\"))\n",
    "vocab_dict = vocab.vocabulary_\n",
    "sparse_matrix_train = scipy.sparse.load_npz(\"review_text_features_countvec/review_text_train_vec.npz\")\n",
    "sparse_matrix_test = scipy.sparse.load_npz(\"review_text_features_countvec/review_text_test_vec.npz\")\n",
    "\n",
    "# doc2vec 50, 100, 200 features vector for training\n",
    "d2v_50_train = pd.read_csv(r\"review_text_features_doc2vec50/review_text_train_doc2vec50.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_100_train = pd.read_csv(r\"review_text_features_doc2vec100/review_text_train_doc2vec100.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_200_train = pd.read_csv(r\"review_text_features_doc2vec200/review_text_train_doc2vec200.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "\n",
    "# doc2vec 50, 100, 200 features vector for testing\n",
    "d2v_50_test = pd.read_csv(r\"review_text_features_doc2vec50/review_text_test_doc2vec50.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_100_test = pd.read_csv(r\"review_text_features_doc2vec100/review_text_test_doc2vec100.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_200_test = pd.read_csv(r\"review_text_features_doc2vec200/review_text_test_doc2vec200.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "sparse_matrix_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "consider in countvectoriser: not using stopwords param, adding lemmatizer param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, choose which predefined and given vector to use to represent the text data.\n",
    "# Text here is already processed\n",
    "\n",
    "d2vtrain = [d2v_50_train, d2v_100_train, d2v_200_train]\n",
    "d2v_50_train.name = \"d2v_50\"\n",
    "d2v_100_train.name = \"d2v_100\"\n",
    "d2v_200_train.name = \"d2v_200\"\n",
    "\n",
    "# Function to split a doc2Vec vector into training and testing\n",
    "def splitDoc2Vec(doc, randomstate = 8579):\n",
    "    # For each of the doc2vec vectors, concat with meta features\n",
    "    train = doc\n",
    "    train = pd.concat([train.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "    \n",
    "    # Above concat method causes duplicating index column\n",
    "    train = train.loc[:,~train.columns.duplicated()]\n",
    "    train = train.drop(columns = ['index'])\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train[train.columns[:-1]],\n",
    "                                                        train[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "    return X_train, X_test, Y_train, np.array(Y_test)\n",
    "\n",
    "# Function to split the given sparse matrix into training and testing\n",
    "def splitSparse(randomstate = 8579):\n",
    "    X_train, X_vali, Y_train, Y_vali = train_test_split(sparse_matrix_train, \n",
    "                                                            train_data[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "    return X_train, X_vali, Y_train, np.array(Y_vali)\n",
    "\n",
    "# Function to split the training data into train and test, but first vectorising the reviews\n",
    "def splitVectorised(ngram_range = (1,1), randomstate = 8579):\n",
    "    # Split the reviews and ratings\n",
    "    X_train, X_vali, Y_train, y_vali = train_test_split(train_data[\"review\"], train_data[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "    X_train_txt, y_train = np.array(X_train), np.array(X_train)\n",
    "    X_test_txt, y_test = np.array(X_vali), np.array(y_vali)\n",
    "        \n",
    "    # vectorise the reviews\n",
    "    vectoriser = CountVectorizer(ngram_range=ngram_range) #optional stop_words = 'english', tokenizer=LemmaTokenizer()\n",
    "    vectoriser.fit(X_train_txt)\n",
    "    X_train = vectoriser.transform(X_train_txt)\n",
    "    X_test = vectoriser.transform(X_test_txt)\n",
    "    \n",
    "    # Need to return vectoriser fit external test set\n",
    "    return X_train, X_test, Y_train, y_test, vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run classifiers using a single train test split to get some crude results. Cross validation comes later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crude accuracy score and confusion matrix of predictions with % correct. Matrix is \"Predicted label x for an instance\n",
    "# with Class label y\"\n",
    "\n",
    "def evaluate(truthlist, predictions):\n",
    "    # First calculate a crude accuracy score\n",
    "    correct = 0;\n",
    "    wrong = 0;\n",
    "    for i in range(0,len(truthlist)):\n",
    "        if(truthlist[i] == predictions[i]):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1;\n",
    "    print(\"The accuracy of the predictions is: {:.5f}\\n\".format(correct/(correct + wrong)))\n",
    "        \n",
    "    # Now construct a confusion matrix of each attribute\n",
    "    truthSeries = pd.Series(truthlist, name = \"Truths\")\n",
    "    predictionSeries = pd.Series(predictions, name = \"Predictions\")\n",
    "    \n",
    "    # Now normalise the confusion matrix so its a percentage of classification performance\n",
    "    confusionDf = pd.crosstab(truthSeries, predictionSeries, rownames=[\"Truths\"], colnames=[\"Predicted\"], margins=False)\n",
    "    confusionDfNormalised = confusionDf / confusionDf.sum(axis=0)\n",
    "    print(\"Confusion Matrix of Correctly Labeled Classes %'s\\n\")\n",
    "    print(confusionDfNormalised)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer if wanted in countvectoriser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alecy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNB\n",
    "MNB runs ~84% with ngrams (1,1). ~72% with ngrams(1,2), ~69% with ngrams (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + f\"Trained with count vectoriser\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali = splitVectorised((1,1))\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)\n",
    "\n",
    "print('\\033[1m' + f\"Trained with sparse matrix\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali = splitSparse()\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with d2v_50 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.82027\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.532189  0.062937  0.006475\n",
      "3          0.276824  0.566434  0.053872\n",
      "5          0.190987  0.370629  0.939653\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with d2v_100 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.83203\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.575107  0.059052  0.006734\n",
      "3          0.244635  0.604507  0.054390\n",
      "5          0.180258  0.336441  0.938876\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with d2v_200 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.83060\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.570815  0.059829  0.008547\n",
      "3          0.244635  0.606838  0.054908\n",
      "5          0.184549  0.333333  0.936545\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with vectorised features\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-ad64504922d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# using count vectoriser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\033[1m'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf\"Trained with vectorised features\"\u001b[0m \u001b[1;34m'\\033[0m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_vali\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitVectorised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'multinomial'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_vali\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Doc 2 vecs\n",
    "for d2v in d2vtrain:\n",
    "    print('\\033[1m' + f\"Trained with {d2v.name} features\" '\\033[0m')\n",
    "    X_train, X_vali, Y_train, Y_vali = splitDoc2Vec(d2v)\n",
    "    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali)\n",
    "    \n",
    "# using count vectoriser\n",
    "print('\\033[1m' + f\"Trained with vectorised features\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali = splitVectorised((1,2))\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM linear kernel with feature selection. not feasible otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2vecs\n",
    "for d2v in d2vtrain:\n",
    "    # Can reduce the time?\n",
    "    print(f\"Training using {d2v.name} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = splitDoc2Vec(d2v)\n",
    "\n",
    "    mi = SelectKBest(score_func=mutual_info_classif, k=50)\n",
    "    mi.fit(X_train, Y_train)\n",
    "    X_train_mi = mi.transform(X_train)\n",
    "    X_test_mi = mi.transform(X_vali)\n",
    "\n",
    "    clf = LinearSVC(max_iter=10000).fit(X_train_mi, Y_train)\n",
    "    Y_pred = clf.predict(X_test_mi)\n",
    "    evaluate(Y_pred, Y_vali)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countvectoriser. takes ~15 minutes for 1k best\n",
    "print(f\"Training using countvectoriser data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = splitVectorised()\n",
    "\n",
    "x2 = SelectKBest(chi2, k=500)\n",
    "X_train_x2 = x2.fit_transform(X_train, Y_train)\n",
    "X_vali_x2 = x2.transform(X_vali)\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0).fit(X_train_x2, Y_train)\n",
    "Y_pred = clf.predict(X_vali_x2)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\alecy\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\alecy\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alecy\\anaconda3\\lib\\site-packages (from xgboost) (1.18.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2vecs\n",
    "for d2v in d2vtrain:\n",
    "    print(f\"Training using {d2v.name} with XGBoost\")\n",
    "    X_train, X_vali, Y_train, Y_vali = splitDoc2Vec(d2v)\n",
    "    clf = XGBClassifier(nrounds=200).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectoriser\n",
    "print(f\"Training using count_vectorised with XGBoost\")\n",
    "X_train, X_vali, Y_train, Y_vali = splitVectorised()\n",
    "clf = XGBClassifier().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)\n",
    "\n",
    "#Sparse\n",
    "print(f\"Training using sparse with XGBoost\")\n",
    "X_train, X_vali, Y_train, Y_vali = splitSparse()\n",
    "clf = XGBClassifier().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest with feature selection.\n",
    "much better with feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d2v in d2vtrain:\n",
    "    print(f\"Training using {d2v.name} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = splitDoc2Vec(d2v)\n",
    "\n",
    "    mi = SelectKBest(score_func=mutual_info_classif, k=(int)(len(d2v.columns)/2))\n",
    "    mi.fit(X_train, Y_train)\n",
    "    X_train_mi = mi.transform(X_train)\n",
    "    X_test_mi = mi.transform(X_vali)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100).fit(X_train_mi, Y_train)\n",
    "    Y_pred = clf.predict(X_test_mi)\n",
    "    evaluate(Y_pred, Y_vali)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using count_vectorised data set\n",
      "The accuracy of the predictions is: 0.79498\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.291845  0.007770  0.000518\n",
      "3          0.210300  0.422688  0.019684\n",
      "5          0.497854  0.569542  0.979798\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorised\n",
    "print(f\"Training using count_vectorised data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = splitVectorised()\n",
    "    \n",
    "x2 = SelectKBest(chi2, k=500)\n",
    "X_train_x2 = x2.fit_transform(X_train, Y_train)\n",
    "X_vali_x2 = x2.transform(X_vali)\n",
    "    \n",
    "clf = RandomForestClassifier().fit(X_train_x2, Y_train)\n",
    "Y_pred = clf.predict(X_vali_x2)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION FOR ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "classifiers = {\n",
    "    #\"MNB\": MultinomialNB(),\n",
    "    \"RFC\": RandomForestClassifier(max_depth = 50),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"LR\": LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1),\n",
    "    \"SVMLINEAR\":  SVC(kernel='linear', C=1.0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using Doc2vecs. Note that MNB doesn't work here as there are negative and continuous values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING ON d2v_50: \n",
      "\n",
      "                RFC took 14.20 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.780\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 14.00 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.774\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 13.96 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.777\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 14.57 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.773\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 15.31 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.773\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for RFC was 0.776\n",
      "TESTING ON d2v_50: \n",
      "\n",
      "                XGB took 23.94 seconds to train\n",
      "                and has training accuracy 0.984 and testing accuracy 0.814\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 26.24 seconds to train\n",
      "                and has training accuracy 0.982 and testing accuracy 0.815\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 25.95 seconds to train\n",
      "                and has training accuracy 0.982 and testing accuracy 0.817\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 27.08 seconds to train\n",
      "                and has training accuracy 0.984 and testing accuracy 0.815\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 29.11 seconds to train\n",
      "                and has training accuracy 0.982 and testing accuracy 0.815\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for XGB was 0.815\n",
      "TESTING ON d2v_50: \n",
      "\n",
      "                LR took 1.32 seconds to train\n",
      "                and has training accuracy 0.824 and testing accuracy 0.816\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 1.28 seconds to train\n",
      "                and has training accuracy 0.823 and testing accuracy 0.823\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 0.95 seconds to train\n",
      "                and has training accuracy 0.824 and testing accuracy 0.820\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 0.83 seconds to train\n",
      "                and has training accuracy 0.821 and testing accuracy 0.829\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 0.89 seconds to train\n",
      "                and has training accuracy 0.824 and testing accuracy 0.819\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for LR was 0.822\n",
      "TESTING ON d2v_50: \n",
      "\n",
      "                SVMLINEAR took 114.80 seconds to train\n",
      "                and has training accuracy 0.825 and testing accuracy 0.814\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                SVMLINEAR took 76.93 seconds to train\n",
      "                and has training accuracy 0.824 and testing accuracy 0.824\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                SVMLINEAR took 79.45 seconds to train\n",
      "                and has training accuracy 0.823 and testing accuracy 0.823\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                SVMLINEAR took 83.80 seconds to train\n",
      "                and has training accuracy 0.822 and testing accuracy 0.827\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                SVMLINEAR took 83.50 seconds to train\n",
      "                and has training accuracy 0.824 and testing accuracy 0.820\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for SVMLINEAR was 0.822\n",
      "TESTING ON d2v_100: \n",
      "\n",
      "                RFC took 25.19 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.755\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 25.43 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.755\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 26.53 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.755\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 26.17 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.751\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                RFC took 24.66 seconds to train\n",
      "                and has training accuracy 1.000 and testing accuracy 0.748\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for RFC was 0.753\n",
      "TESTING ON d2v_100: \n",
      "\n",
      "                XGB took 54.71 seconds to train\n",
      "                and has training accuracy 0.993 and testing accuracy 0.811\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 55.12 seconds to train\n",
      "                and has training accuracy 0.992 and testing accuracy 0.819\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 55.91 seconds to train\n",
      "                and has training accuracy 0.991 and testing accuracy 0.812\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 53.07 seconds to train\n",
      "                and has training accuracy 0.992 and testing accuracy 0.811\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                XGB took 61.12 seconds to train\n",
      "                and has training accuracy 0.992 and testing accuracy 0.814\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for XGB was 0.813\n",
      "TESTING ON d2v_100: \n",
      "\n",
      "                LR took 1.20 seconds to train\n",
      "                and has training accuracy 0.836 and testing accuracy 0.827\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 0.98 seconds to train\n",
      "                and has training accuracy 0.835 and testing accuracy 0.829\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 0.84 seconds to train\n",
      "                and has training accuracy 0.832 and testing accuracy 0.838\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 1.24 seconds to train\n",
      "                and has training accuracy 0.834 and testing accuracy 0.828\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                LR took 1.29 seconds to train\n",
      "                and has training accuracy 0.834 and testing accuracy 0.830\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "the mean test score for LR was 0.830\n",
      "TESTING ON d2v_100: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                SVMLINEAR took 189.04 seconds to train\n",
      "                and has training accuracy 0.837 and testing accuracy 0.828\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n",
      "\n",
      "                SVMLINEAR took 185.77 seconds to train\n",
      "                and has training accuracy 0.836 and testing accuracy 0.830\n",
      "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "for d2v in d2vtrain:\n",
    "    \n",
    "    # Include the meta features with the doc2vec\n",
    "    dataframe = pd.concat([d2v.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "    # Above concat method causes duplicating index column\n",
    "    dataframe = dataframe.loc[:,~dataframe.columns.duplicated()]\n",
    "    dataframe = dataframe.drop(columns = ['index'])\n",
    "    \n",
    "    train = shuffle(dataframe, random_state = 0)\n",
    "    folds = np.array_split(train, 5)\n",
    "    \n",
    "    for classifier in list(classifiers.keys()):\n",
    "        print(f\"TESTING ON {d2v.name}: \")\n",
    "        clf = classifiers[classifier]\n",
    "\n",
    "        test_scores = []\n",
    "\n",
    "        for fold_n in range(5):\n",
    "\n",
    "            train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "            train_fold = pd.concat(train_folds, axis = 0)\n",
    "            test_fold = folds[fold_n]\n",
    "\n",
    "            # split and train the model. Include all columns other than rating in training\n",
    "            X_train, y_train = train_fold[train_fold.columns.difference([\"rating\"])], np.array(train_fold[\"rating\"])\n",
    "            X_test, y_test = test_fold[train_fold.columns.difference([\"rating\"])], np.array(test_fold[\"rating\"])\n",
    "\n",
    "            # Record some stats\n",
    "            before = time.time()\n",
    "            clf.fit(X_train, y_train)\n",
    "            after = time.time()\n",
    "            train_time = after - before\n",
    "\n",
    "            train_score = clf.score(X_train, y_train)\n",
    "            test_score = clf.score(X_test, y_test)\n",
    "\n",
    "            test_scores.append(test_score)\n",
    "\n",
    "            print(\n",
    "                '''\n",
    "                {} took {:.2f} seconds to train\n",
    "                and has training accuracy {:.3f} and testing accuracy {:.3f}\n",
    "                - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "                '''.format(classifier, train_time, train_score, test_score)\n",
    "            )\n",
    "\n",
    "        mean_test_score = np.mean(test_scores)\n",
    "\n",
    "        print(\"the mean test score for {} was {:.3f}\".format(classifier, mean_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training using count_vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffle(train_data[['rating', 'review']], random_state = 0)\n",
    "folds = np.array_split(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING ON COUNTVECTORIZER\n",
      "\n",
      "            RFC took 36.19 seconds to train\n",
      "            and has training accuracy 0.987 and testing accuracy 0.786\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            RFC took 36.41 seconds to train\n",
      "            and has training accuracy 0.987 and testing accuracy 0.784\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            RFC took 36.90 seconds to train\n",
      "            and has training accuracy 0.987 and testing accuracy 0.786\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            RFC took 36.54 seconds to train\n",
      "            and has training accuracy 0.987 and testing accuracy 0.781\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            RFC took 37.18 seconds to train\n",
      "            and has training accuracy 0.987 and testing accuracy 0.780\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for RFC was 0.783\n",
      "\n",
      "            XGB took 32.18 seconds to train\n",
      "            and has training accuracy 0.953 and testing accuracy 0.856\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            XGB took 33.60 seconds to train\n",
      "            and has training accuracy 0.952 and testing accuracy 0.853\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            XGB took 31.48 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.845\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            XGB took 33.44 seconds to train\n",
      "            and has training accuracy 0.952 and testing accuracy 0.849\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            XGB took 29.98 seconds to train\n",
      "            and has training accuracy 0.952 and testing accuracy 0.844\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for XGB was 0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LR took 4.41 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.862\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LR took 4.43 seconds to train\n",
      "            and has training accuracy 0.948 and testing accuracy 0.858\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LR took 4.01 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.857\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LR took 4.46 seconds to train\n",
      "            and has training accuracy 0.950 and testing accuracy 0.855\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LR took 4.94 seconds to train\n",
      "            and has training accuracy 0.954 and testing accuracy 0.859\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "the mean test score for LR was 0.858\n",
      "\n",
      "            SVMLINEAR took 618.65 seconds to train\n",
      "            and has training accuracy 0.995 and testing accuracy 0.842\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n",
      "\n",
      "            SVMLINEAR took 600.04 seconds to train\n",
      "            and has training accuracy 0.994 and testing accuracy 0.839\n",
      "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING ON COUNTVECTORIZER\")\n",
    "\n",
    "for classifier in list(classifiers.keys()):\n",
    "    \n",
    "    clf = classifiers[classifier]\n",
    "    \n",
    "    test_scores = []\n",
    "    \n",
    "    for fold_n in range(5):\n",
    "        \n",
    "        train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "        train_fold = pd.concat(train_folds, axis = 0)\n",
    "        test_fold = folds[fold_n]\n",
    "\n",
    "        X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "        X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])\n",
    "        \n",
    "        vectorser = CountVectorizer(ngram_range=(1, 1))\n",
    "        vectoriser.fit(X_train_txt)\n",
    "        X_train = vectoriser.transform(X_train_txt)\n",
    "        X_test = vectoriser.transform(X_test_txt)\n",
    "        \n",
    "        # If its the RFC, optimise it by feature selecting\n",
    "        if(classifier == \"RFC\"):\n",
    "            x2 = SelectKBest(chi2, k=1000)\n",
    "            X_train = x2.fit_transform(X_train, y_train)\n",
    "            X_test = x2.transform(X_test)\n",
    "\n",
    "        before = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        after = time.time()\n",
    "        train_time = after - before\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "        print(\n",
    "            '''\n",
    "            {} took {:.2f} seconds to train\n",
    "            and has training accuracy {:.3f} and testing accuracy {:.3f}\n",
    "            - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "            '''.format(classifier, train_time, train_score, test_score)\n",
    "        )\n",
    "    \n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    \n",
    "    print(\"the mean test score for {} was {:.3f}\".format(classifier, mean_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "seems like the doc2vec vectors aren't as informative as the countVectoriser  \n",
    "best to use ngrams(1,1) for doc2vecs? (1,2) causes marginal difference in other models, but causes around a 10% accuracy dip for multinomial  \n",
    "RFC gains around 7-8% accuracy if trained using chi2 features  \n",
    "of course, chi2 can't be used for d2v inputs as they contain negative values  \n",
    "MNB also can't be used for d2v inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All except MNB:  \n",
    "first do for d2v_50_train  84~  \n",
    "stacking ~ 85% for 50  \n",
    "86.2% for 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "RFC accuracy 0.7278232988956181\n",
      "ADA accuracy 0.7502671891699323\n",
      "XGB accuracy 0.8120769504809405\n",
      "LR accuracy 0.8350552190951194\n",
      "SVMLINEAR accuracy 0.8339864624153901\n",
      "TIME:  706.2092406749725\n",
      "stacking accuracy is 0.8626193529998575\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-9f7f072d91ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mclfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} accuracy {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 392\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectoriser = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "scores = []\n",
    "\n",
    "classifiers = {\n",
    "    #\"MNB\": MultinomialNB(),\n",
    "    \"RFC\": RandomForestClassifier(max_depth = 50),\n",
    "    \"ADA\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"LR\": LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1),\n",
    "    \"SVMLINEAR\":  SVC(kernel='linear', C=1.0)\n",
    "}\n",
    "\n",
    "# Include the meta features with the doc2vec\n",
    "dataframe = pd.concat([d2v_200_train.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "# Above concat method causes duplicating index column\n",
    "dataframe = dataframe.loc[:,~dataframe.columns.duplicated()]\n",
    "dataframe = dataframe.drop(columns = ['index'])\n",
    "    \n",
    "train = shuffle(dataframe, random_state = 0)\n",
    "folds = np.array_split(train, 5)\n",
    "\n",
    "\n",
    "# loop\n",
    "\n",
    "for fold_n in range(5):\n",
    "    \n",
    "    print(fold_n)\n",
    "    \n",
    "    train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "    train_fold = pd.concat(train_folds, axis = 0)\n",
    "    test_fold = folds[fold_n]\n",
    "    \n",
    "    meta_train, meta_test = train_test_split(dataframe)\n",
    "    \n",
    "    # split and train the model. Include all columns other than rating in training\n",
    "    X_train, y_train = train_fold[train_fold.columns.difference([\"rating\"])], np.array(train_fold[\"rating\"])\n",
    "    X_test, y_test = test_fold[train_fold.columns.difference([\"rating\"])], np.array(test_fold[\"rating\"])\n",
    "\n",
    "    before = time.time()\n",
    "    clfs = []\n",
    "    predictions_list = []\n",
    "    \n",
    "    for classifier in list(classifiers.keys()):\n",
    "\n",
    "        clf = classifiers[classifier]\n",
    "        clfs.append(clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(\"{} accuracy {}\".format(classifier, score))\n",
    "        predictions = clf.predict(X_test)\n",
    "        predictions_list.append(predictions)\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"TIME: \", after - before)\n",
    "    meta_clf = RandomForestClassifier()\n",
    "    meta_clf.fit(np.stack(predictions_list).transpose(), y_test)\n",
    "\n",
    "    # lets test the meta classifier on the meta test data\n",
    "\n",
    "    reviews = meta_test\n",
    "    new_predictions = []\n",
    "    \n",
    "    # Predict the test set without rating column\n",
    "    for clf in clfs:\n",
    "        predictions = clf.predict(meta_test[meta_test.columns.difference([\"rating\"])])\n",
    "        new_predictions.append(predictions)\n",
    "\n",
    "    score = meta_clf.score(np.stack(new_predictions).transpose(), meta_test.rating)\n",
    "    scores.append(score)\n",
    "    print(\"stacking accuracy is {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8459455607809605"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try for splitVectorised()\n",
    "same as mike, but small changes to parameters in model. also trains random forest classifier differently to others (feature selection)   \n",
    "Using meta_clf = Logit:  \n",
    "all except mnb give 86.5  \n",
    "all except svm give 86.7, with lemmatokenizer and stop words 85%   \n",
    "all except mnb and svm gives 86.6    \n",
    "all except rfc = 86.5  \n",
    "using metaclf = XBG:  \n",
    "all except svm sometimes 77 most times 87.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MNB accuracy 0.7537402042270245\n",
      "RFC accuracy 0.7964853953930183\n",
      "ADA accuracy 0.7891237235810972\n",
      "XGB accuracy 0.8537164568985989\n",
      "LR accuracy 0.8727143196390406\n",
      "SVMLINEAR accuracy 0.865590121111375\n",
      "TIME 788.207319021225\n",
      "stacking accuracy is 0.8540686903235001\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-2d91e5bf0203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mX_test_txt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_fold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_fold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rating\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mvectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \"\"\"\n\u001b[0;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m                                                        max_features)\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[1;34m(self, X, vocabulary)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \"\"\"\n\u001b[0;32m   1042\u001b[0m         \u001b[0msorted_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[0mmap_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectoriser = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "scores = []\n",
    "\n",
    "classifiers = {\n",
    "    \"MNB\": MultinomialNB(),\n",
    "    \"RFC\": RandomForestClassifier(max_depth = 50),\n",
    "    \"ADA\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"LR\": LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1),\n",
    "    \"SVMLINEAR\":  SVC(kernel='linear', C=0.1)\n",
    "}\n",
    "\n",
    "# loop\n",
    "\n",
    "train = shuffle(train_data[['rating', 'review']], random_state = 0)\n",
    "folds = np.array_split(train, 5)\n",
    "\n",
    "for fold_n in range(5):\n",
    "    \n",
    "    print(fold_n)\n",
    "    \n",
    "    meta_train, meta_test = train_test_split(train_data[['rating', 'review']])\n",
    "    train = shuffle(meta_train)\n",
    "    \n",
    "    folds = np.array_split(train, 5)\n",
    "\n",
    "    train_folds = [folds[i] for i in range(len(folds)) if i != fold_n]\n",
    "    train_fold = pd.concat(train_folds, axis = 0)\n",
    "    test_fold = folds[fold_n]\n",
    "\n",
    "    X_train_txt, y_train = train_fold[\"review\"], np.array(train_fold[\"rating\"])\n",
    "    X_test_txt, y_test = test_fold[\"review\"], np.array(test_fold[\"rating\"])\n",
    "\n",
    "    vectoriser.fit(X_train_txt)\n",
    "    X_train = vectoriser.transform(X_train_txt)\n",
    "    X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "    before = time.time()\n",
    "    \n",
    "    clfs = []\n",
    "    \n",
    "    predictions_list = []\n",
    "    \n",
    "    for classifier in list(classifiers.keys()):\n",
    "\n",
    "        clf = classifiers[classifier]\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        # could so some feature selection here potentially, or train separate models differently\n",
    "        if(\"RFC\" in classifiers and classifier == \"RFC\"):\n",
    "            # Optimise RFC with feature selection. Train the model using these k features\n",
    "            x2 = SelectKBest(chi2, k=1000)\n",
    "            X_train_rfc = x2.fit_transform(X_train, y_train)\n",
    "            X_test_rfc = x2.transform(X_test)\n",
    "            \n",
    "            clf.fit(X_train_rfc, y_train)\n",
    "            \n",
    "            score = clf.score(X_test_rfc, y_test)\n",
    "            print(\"{} accuracy {}\".format(classifier, score))\n",
    "            \n",
    "            predictions = clf.predict(X_test_rfc)\n",
    "            \n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            print(\"{} accuracy {}\".format(classifier, score))\n",
    "            predictions = clf.predict(X_test)\n",
    "            \n",
    "        predictions_list.append(predictions)\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"TIME\", after - before)\n",
    "    \n",
    "    #meta_clf = RandomForestClassifier()\n",
    "    #meta_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    #meta_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1)\n",
    "    #meta_clf = SVC(kernel='linear', C=1.0)\n",
    "    meta_clf = XGBClassifier()\n",
    "    \n",
    "    meta_clf.fit(np.stack(predictions_list).transpose(), y_test)\n",
    "\n",
    "    # lets test the meta classifier on the meta test data\n",
    "\n",
    "    reviews = vectoriser.transform(meta_test.review)\n",
    "    new_predictions = []\n",
    "    \n",
    "    for clf in clfs:\n",
    "        if(\"RFC\" in classifiers and clf == classifiers[\"RFC\"]):\n",
    "            # Select best k features in meta_test\n",
    "            x2 = SelectKBest(chi2, k=1000)\n",
    "            reviews_rfc = x2.fit_transform(reviews, meta_test.rating)\n",
    "            predictions = clf.predict(reviews_rfc)\n",
    "        else:\n",
    "            predictions = clf.predict(reviews)\n",
    "            \n",
    "        new_predictions.append(predictions)\n",
    "\n",
    "    score = meta_clf.score(np.stack(new_predictions).transpose(), meta_test.rating)\n",
    "    scores.append(score)\n",
    "    print(\"stacking accuracy is {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try some kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with vectorised features\u001b[0m\n",
      "The accuracy of the predictions is: 0.87068\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.669528  0.053613  0.003885\n",
      "3          0.193133  0.692308  0.041699\n",
      "5          0.137339  0.254079  0.954416\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just normal Logistic regression model\n",
    "# using count vectoriser\n",
    "print('\\033[1m' + f\"Trained with vectorised features\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali, vectoriser = splitVectorised((1,2))\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, ..., 3, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectoriser.transform(test_text.review)\n",
    "Y_pred = clf.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>6989</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>6990</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>6991</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>6992</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>6993</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>6994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>6995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>6996</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>6997</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>6998</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>6999</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>7000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>7001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>7002</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>7003</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>7004</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>7005</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>7006</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>7007</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>7008</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>7009</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>7010</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>7011</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>7012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>7013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>7014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>7015</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>7016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>7017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>7018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id  rating\n",
       "0               1       5\n",
       "1               2       5\n",
       "2               3       1\n",
       "3               4       3\n",
       "4               5       5\n",
       "5               6       5\n",
       "6               7       5\n",
       "7               8       5\n",
       "8               9       5\n",
       "9              10       5\n",
       "10             11       5\n",
       "11             12       5\n",
       "12             13       5\n",
       "13             14       5\n",
       "14             15       5\n",
       "15             16       1\n",
       "16             17       5\n",
       "17             18       5\n",
       "18             19       5\n",
       "19             20       5\n",
       "20             21       5\n",
       "21             22       1\n",
       "22             23       5\n",
       "23             24       3\n",
       "24             25       3\n",
       "25             26       5\n",
       "26             27       5\n",
       "27             28       3\n",
       "28             29       5\n",
       "29             30       3\n",
       "...           ...     ...\n",
       "6988         6989       5\n",
       "6989         6990       5\n",
       "6990         6991       5\n",
       "6991         6992       3\n",
       "6992         6993       3\n",
       "6993         6994       3\n",
       "6994         6995       5\n",
       "6995         6996       5\n",
       "6996         6997       5\n",
       "6997         6998       5\n",
       "6998         6999       5\n",
       "6999         7000       3\n",
       "7000         7001       5\n",
       "7001         7002       5\n",
       "7002         7003       5\n",
       "7003         7004       5\n",
       "7004         7005       5\n",
       "7005         7006       5\n",
       "7006         7007       5\n",
       "7007         7008       5\n",
       "7008         7009       5\n",
       "7009         7010       5\n",
       "7010         7011       5\n",
       "7011         7012       1\n",
       "7012         7013       3\n",
       "7013         7014       5\n",
       "7014         7015       5\n",
       "7015         7016       3\n",
       "7016         7017       5\n",
       "7017         7018       5\n",
       "\n",
       "[7018 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsdf = pd.DataFrame(list(enumerate(Y_pred, start = 1)))\n",
    "predictionsdf = predictionsdf.rename(columns = {0: 'instance_id', 1: 'rating'})\n",
    "predictionsdf.to_csv(\"preds1.csv\", index = False)\n",
    "predictionsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy 0.8818583440216616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=1000, multi_class='multinomial')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the stacking model now\n",
    "\n",
    "vectoriser = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "scores = []\n",
    "\n",
    "classifiers = {\n",
    "    #\"MNB\": MultinomialNB(),\n",
    "    #\"RFC\": RandomForestClassifier(max_depth = 50),\n",
    "    #\"ADA\": AdaBoostClassifier(),\n",
    "    #\"XGB\": XGBClassifier(),\n",
    "    \"LR\": LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1)\n",
    "    #\"SVMLINEAR\":  SVC(kernel='linear', C=1.0)\n",
    "}\n",
    "\n",
    "# loop\n",
    "\n",
    "\n",
    "meta_train, meta_test = train_test_split(train_data[['rating', 'review']])\n",
    "train = shuffle(meta_train)\n",
    "    \n",
    "vectoriser.fit(X_train_txt)\n",
    "X_train = vectoriser.transform(meta_train.review)\n",
    "Y_train = meta_train.rating\n",
    "\n",
    "X_test = vectoriser.transform(meta_test.review)\n",
    "y_test = meta_test.rating\n",
    "    \n",
    "clfs = []\n",
    "\n",
    "predictions_list = []\n",
    "    \n",
    "for classifier in list(classifiers.keys()):\n",
    "\n",
    "    clf = classifiers[classifier]\n",
    "    clfs.append(clf)\n",
    "        \n",
    "    # could so some feature selection here potentially, or train separate models differently\n",
    "    if(\"RFC\" in classifiers and classifier == \"RFC\"):\n",
    "        # Optimise RFC with feature selection. Train the model using these k features\n",
    "        x2 = SelectKBest(chi2, k=1000)\n",
    "        X_train_rfc = x2.fit_transform(X_train, y_train)\n",
    "        X_test_rfc = x2.transform(X_test)\n",
    "            \n",
    "        clf.fit(X_train_rfc, Y_train)\n",
    "            \n",
    "        score = clf.score(X_test_rfc, y_test)\n",
    "        print(\"{} accuracy {}\".format(classifier, score))\n",
    "            \n",
    "        predictions = clf.predict(X_test_rfc)\n",
    "            \n",
    "    else:\n",
    "        clf.fit(X_train, Y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(\"{} accuracy {}\".format(classifier, score))\n",
    "        predictions = clf.predict(X_test)\n",
    "            \n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "#meta_clf = RandomForestClassifier()\n",
    "#meta_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "meta_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, C = 0.1)\n",
    "#meta_cld = SVC(kernel='linear', C=1.0)\n",
    "    \n",
    "meta_clf.fit(np.stack(predictions_list).transpose(), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectoriser.transform(test_text.review)\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for classifier in list(classifiers.keys()):\n",
    "\n",
    "    clf = classifiers[classifier]\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    predictions_list.append(predictions)\n",
    "    \n",
    "predictions = meta_clf.predict(np.stack(predictions_list).transpose())\n",
    "\n",
    "predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>6989</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>6990</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>6991</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>6992</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>6993</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>6994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>6995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>6996</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>6997</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>6998</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>6999</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>7000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>7001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>7002</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>7003</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>7004</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>7005</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>7006</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>7007</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>7008</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>7009</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>7010</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>7011</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>7012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>7013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>7014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>7015</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>7016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>7017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>7018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id  rating\n",
       "0               1       5\n",
       "1               2       5\n",
       "2               3       1\n",
       "3               4       3\n",
       "4               5       5\n",
       "5               6       5\n",
       "6               7       5\n",
       "7               8       5\n",
       "8               9       5\n",
       "9              10       5\n",
       "10             11       5\n",
       "11             12       5\n",
       "12             13       5\n",
       "13             14       5\n",
       "14             15       5\n",
       "15             16       1\n",
       "16             17       5\n",
       "17             18       5\n",
       "18             19       5\n",
       "19             20       5\n",
       "20             21       5\n",
       "21             22       1\n",
       "22             23       5\n",
       "23             24       3\n",
       "24             25       3\n",
       "25             26       5\n",
       "26             27       5\n",
       "27             28       3\n",
       "28             29       5\n",
       "29             30       3\n",
       "...           ...     ...\n",
       "6988         6989       5\n",
       "6989         6990       5\n",
       "6990         6991       5\n",
       "6991         6992       3\n",
       "6992         6993       3\n",
       "6993         6994       3\n",
       "6994         6995       5\n",
       "6995         6996       5\n",
       "6996         6997       5\n",
       "6997         6998       5\n",
       "6998         6999       5\n",
       "6999         7000       3\n",
       "7000         7001       5\n",
       "7001         7002       5\n",
       "7002         7003       5\n",
       "7003         7004       5\n",
       "7004         7005       5\n",
       "7005         7006       5\n",
       "7006         7007       5\n",
       "7007         7008       5\n",
       "7008         7009       5\n",
       "7009         7010       5\n",
       "7010         7011       5\n",
       "7011         7012       1\n",
       "7012         7013       3\n",
       "7013         7014       5\n",
       "7014         7015       5\n",
       "7015         7016       3\n",
       "7016         7017       5\n",
       "7017         7018       5\n",
       "\n",
       "[7018 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsdf = pd.DataFrame(list(enumerate(Y_pred, start = 1)))\n",
    "predictionsdf = predictionsdf.rename(columns = {0: 'instance_id', 1: 'rating'})\n",
    "predictionsdf.to_csv(\"preds2.csv\", index = False)\n",
    "predictionsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
