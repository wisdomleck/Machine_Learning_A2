{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "#Natural language toolkit. Download if not installed already\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# For splitting by punctuation and using regex\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Useful\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Potentially used Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Evaluation and feature selection tools\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote_funny</th>\n",
       "      <th>vote_cool</th>\n",
       "      <th>vote_useful</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>dear longman &amp; eagle.......you've left me no c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Delish. The hubby and I wanted to do brunch on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yep, I've giving Yolk 5 stars. It's just reall...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Meat, meat, meat. It's meat-tastic. So much me...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I caught up with the law school girls on a Sat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vote_funny  vote_cool  vote_useful  \\\n",
       "0           0          1            3   \n",
       "1           0          0            0   \n",
       "2           1          0            1   \n",
       "3          17          3            3   \n",
       "4           0          0            0   \n",
       "\n",
       "                                              review  rating  \n",
       "0  dear longman & eagle.......you've left me no c...       1  \n",
       "1  Delish. The hubby and I wanted to do brunch on...       5  \n",
       "2  yep, I've giving Yolk 5 stars. It's just reall...       5  \n",
       "3  Meat, meat, meat. It's meat-tastic. So much me...       3  \n",
       "4  I caught up with the law school girls on a Sat...       3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test not useful right now\n",
    "test_meta = pd.read_csv(\"review_meta_test.csv\", sep=\",\", names=[\"label\", \"body_text\"])\n",
    "test_text = pd.read_csv(\"review_meta_train.csv\", sep=',', names=[\"label\", \"body_text\"])\n",
    "\n",
    "# Also not useful right now\n",
    "train_meta = pd.read_csv(\"review_meta_train.csv\", sep=\",\")\n",
    "train_text = pd.read_csv(\"review_text_train.csv\", sep=\",\")\n",
    "\n",
    "# Ids are not predictive. Consider adding in the date?\n",
    "train_meta = train_meta.drop([\"date\", \"review_id\", \"reviewer_id\", \"business_id\"], axis = 1)\n",
    "\n",
    "# Combine the meta features and text. Move class to end\n",
    "train_data = pd.concat([train_meta, train_text], axis=1)\n",
    "class_col = train_data.pop(\"rating\")\n",
    "train_data[\"rating\"] = class_col\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the review text. Extension after building models on vectors provided\n",
    "We want to remove punctuation and numbers, tokenise every review's words, lemmatise the words, then remove all the words in the stopwords list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will take extremely long to execute. Might be useful later if we're generating our own features.\n",
    "# Make sure to run once and export to csv\n",
    "# Just use the vectors provided for now\n",
    "\n",
    "punct = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Preprocesses each review in the dataframe\n",
    "def clean_review(review):\n",
    "    # Reassign the string after every change\n",
    "    \n",
    "    # Remove puncutation and numbers\n",
    "    no_punct = ''.join(char for char in review if char not in punct and not char.isdigit())\n",
    "    \n",
    "    # Tokenize into words\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    words = tokenizer.tokenize(no_punct)\n",
    "    \n",
    "    # Remove stopwords and lemmatize, move to lower case\n",
    "    no_stopwords = [wordnet.lemmatize(word).lower() for word in words if word not in stopwords]\n",
    "    \n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote_funny</th>\n",
       "      <th>vote_cool</th>\n",
       "      <th>vote_useful</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[dear, longman, eagleyouve, left, choice, the,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[delish, the, hubby, i, wanted, brunch, sunday...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yep, I've giving Yolk 5 stars. It's just reall...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Meat, meat, meat. It's meat-tastic. So much me...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I caught up with the law school girls on a Sat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vote_funny  vote_cool  vote_useful  \\\n",
       "0           0          1            3   \n",
       "1           0          0            0   \n",
       "2           1          0            1   \n",
       "3          17          3            3   \n",
       "4           0          0            0   \n",
       "\n",
       "                                              review  rating  \n",
       "0  [dear, longman, eagleyouve, left, choice, the,...       1  \n",
       "1  [delish, the, hubby, i, wanted, brunch, sunday...       5  \n",
       "2  yep, I've giving Yolk 5 stars. It's just reall...       5  \n",
       "3  Meat, meat, meat. It's meat-tastic. So much me...       3  \n",
       "4  I caught up with the law school girls on a Sat...       3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean each review in the dataframe? Takes ~10 mins for all reviews\n",
    "train_data.loc[:1,\"review\"] = train_data.review.apply(lambda x : clean_review(x))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using already preprocessed vectors for the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'review_text_features_countvec/train_countvectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-db6e1462f5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# countvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"review_text_features_countvec/train_countvectorizer.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mvocab_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msparse_matrix_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"review_text_features_countvec/review_text_train_vec.npz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'review_text_features_countvec/train_countvectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "# OR we can just use the preprocessed text data\n",
    "\n",
    "# countvec\n",
    "import pickle\n",
    "vocab = pickle.load(open(\"review_text_features_countvec/train_countvectorizer.pkl\", \"rb\"))\n",
    "vocab_dict = vocab.vocabulary_\n",
    "sparse_matrix_train = scipy.sparse.load_npz(\"review_text_features_countvec/review_text_train_vec.npz\")\n",
    "sparse_matrix_test = scipy.sparse.load_npz(\"review_text_features_countvec/review_text_test_vec.npz\")\n",
    "\n",
    "# doc2vec 50, 100, 200 features vector for training\n",
    "d2v_50_train = pd.read_csv(r\"review_text_features_doc2vec50/review_text_train_doc2vec50.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_100_train = pd.read_csv(r\"review_text_features_doc2vec100/review_text_train_doc2vec100.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_200_train = pd.read_csv(r\"review_text_features_doc2vec200/review_text_train_doc2vec200.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "\n",
    "# doc2vec 50, 100, 200 features vector for testing\n",
    "d2v_50_test = pd.read_csv(r\"review_text_features_doc2vec50/review_text_test_doc2vec50.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_100_test = pd.read_csv(r\"review_text_features_doc2vec100/review_text_test_doc2vec100.csv\", index_col = False, delimiter = \",\", header=None)\n",
    "d2v_200_test = pd.read_csv(r\"review_text_features_doc2vec200/review_text_test_doc2vec200.csv\", index_col = False, delimiter = \",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and form train and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, choose which predefined and given vector to use to represent the text data.\n",
    "# Text here is already processed\n",
    "\n",
    "# Function to combine meta features (optional?)  \n",
    "def preprocess(type, randomstate = 8579):\n",
    "    # For each of the doc2vec vectors, concat with meta features\n",
    "    \n",
    "    if type == \"50\":\n",
    "        train = d2v_50_train\n",
    "        #train = pd.concat([features.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "        \n",
    "    elif type == \"100\" :\n",
    "        train = d2v_100_train\n",
    "        #train = pd.concat([features.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "        \n",
    "    elif type == \"200\" :\n",
    "        train = d2v_200_train\n",
    "        #train = pd.concat([features.reset_index(), train_meta.reset_index()], axis = 1)\n",
    "    \n",
    "    # If we chose a doc2vec vector, split and return the training and validation sets\n",
    "    if type != \"count\" and type != \"vectoriser\":\n",
    "        # Above concat method causes duplicating index column\n",
    "        train = train.loc[:,~train.columns.duplicated()]\n",
    "        \n",
    "        X_train, X_vali, Y_train, Y_vali = train_test_split(train[train.columns[:-1]],\n",
    "                                                        train_meta[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "        return X_train, X_vali, Y_train, Y_vali\n",
    "    \n",
    "    # Otherwise do something with the sparse matrix\n",
    "    elif type == \"count\":\n",
    "        X_train, X_vali, Y_train, Y_vali = train_test_split(sparse_matrix_train, \n",
    "                                                            train_data[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "        return X_train, X_vali, Y_train, Y_vali\n",
    "\n",
    "    # Manually make a count vector, rather than use the sparse matrix\n",
    "    elif type == \"vectoriser\":\n",
    "        # Split the reviews and ratings\n",
    "        X_train, X_vali, Y_train, y_vali = train_test_split(train_data[\"review\"], train_data[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "        X_train_txt, y_train = np.array(X_train), np.array(X_train)\n",
    "        X_test_txt, y_test = np.array(X_vali), np.array(y_vali)\n",
    "\n",
    "        # vectorise the reviews\n",
    "        vectoriser = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "        vectoriser.fit(X_train_txt)\n",
    "        X_train = vectoriser.transform(X_train_txt)\n",
    "        X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "        return X_train, X_test, Y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and train models and Crudely evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crude accuracy score and confusion matrix of predictions with % correct. Matrix is \"Predicted label x for an instance\n",
    "# with Class label y\"\n",
    "\n",
    "def evaluate(truthlist, predictions):\n",
    "    # First calculate a crude accuracy score\n",
    "    correct = 0;\n",
    "    wrong = 0;\n",
    "    for i in range(0,len(truthlist)):\n",
    "        if(truthlist[i] == predictions[i]):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1;\n",
    "    print(\"The accuracy of the predictions is: {:.5f}\\n\".format(correct/(correct + wrong)))\n",
    "        \n",
    "    # Now construct a confusion matrix of each attribute\n",
    "    truthSeries = pd.Series(truthlist, name = \"Truths\")\n",
    "    predictionSeries = pd.Series(predictions, name = \"Predictions\")\n",
    "    \n",
    "    # Now normalise the confusion matrix so its a percentage of classification performance\n",
    "    confusionDf = pd.crosstab(truthSeries, predictionSeries, rownames=[\"Truths\"], colnames=[\"Predicted\"], margins=False)\n",
    "    confusionDfNormalised = confusionDf / confusionDf.sum(axis=0)\n",
    "    print(\"Confusion Matrix of Correctly Labeled Classes %'s\\n\")\n",
    "    print(confusionDfNormalised)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "import csv\n",
    "\n",
    "# Also need a function to export the predictions to a CSV\n",
    "def export(instanceid, predictions):\n",
    "    f = open(\"output.csv\", \"w\", newline='')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Instance_id\", \"rating\"])\n",
    "    \n",
    "    for i, j in zip(instanceid, predictions):\n",
    "        f.write(str(i) + \",\" + str(j))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = [\"1\", \"3\", \"5\"]\n",
    "datasets = [\"50\", \"100\", \"200\", \"count\", \"vectoriser\"] # don't include count for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Naive Bayes\n",
    "Binomial NB would only work for discrete data. Possibly discretise doc2vec vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with count vectoriser\u001b[0m\n",
      "The accuracy of the predictions is: 0.73477\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.334764  0.037296  0.015022\n",
      "3          0.263948  0.418026  0.096348\n",
      "5          0.401288  0.544678  0.888630\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + f\"Trained with count vectoriser\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "clf = BernoulliNB().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive bayes\n",
    "Probably use the doc2vec continuous vectors for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with doc2vec_50 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.72337\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.538627  0.111888  0.052059\n",
      "3          0.190987  0.465423  0.116291\n",
      "5          0.270386  0.422688  0.831650\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_100 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.66726\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.553648  0.150738  0.084952\n",
      "3          0.152361  0.430458  0.155141\n",
      "5          0.293991  0.418803  0.759907\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_200 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.61632\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.538627  0.195027  0.127687\n",
      "3          0.113734  0.379176  0.167573\n",
      "5          0.347639  0.425796  0.704740\n",
      "\n",
      "\n",
      "\n",
      "Haven't used count vector for GNB\n"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    if(i != \"count\"):\n",
    "        print('\\033[1m' + f\"Trained with doc2vec_{i} features\" '\\033[0m')\n",
    "        X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "        clf = GaussianNB().fit(X_train, Y_train)\n",
    "        Y_pred = clf.predict(X_vali)\n",
    "        evaluate(Y_pred, Y_vali.values)\n",
    "    else:\n",
    "        print(\"Haven't used count vector for GNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive bayes\n",
    "Same with binomial Nb. Use the countvector, or discretise the continuous data in doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with count vectoriser\u001b[0m\n",
      "The accuracy of the predictions is: 0.83791\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.583691  0.043512  0.004403\n",
      "3          0.309013  0.605284  0.049469\n",
      "5          0.107296  0.351204  0.946128\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + f\"Trained with count vectoriser\" '\\033[0m')\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"vectoriser\")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "works and runs for all of the vectors provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with doc2vec_50 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.81332\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.480687  0.060606  0.005957\n",
      "3          0.304721  0.563326  0.057239\n",
      "5          0.214592  0.376068  0.936804\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_100 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.82508\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.532189  0.055167  0.007252\n",
      "3          0.251073  0.595183  0.055685\n",
      "5          0.216738  0.349650  0.937063\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_200 features\u001b[0m\n",
      "The accuracy of the predictions is: 0.82829\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.553648  0.064491  0.008547\n",
      "3          0.242489  0.599845  0.053872\n",
      "5          0.203863  0.335664  0.937581\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_count features\u001b[0m\n",
      "The accuracy of the predictions is: 0.84040\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.663090  0.056721  0.006475\n",
      "3          0.214592  0.637918  0.064232\n",
      "5          0.122318  0.305361  0.929293\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTrained with doc2vec_vectoriser features\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-1fee007f7c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'multinomial'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_vali\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_vali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    print('\\033[1m' + f\"Trained with doc2vec_{i} features\" '\\033[0m')\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each SVM Takes around 15-20 minutes to run...\n",
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = LinearSVC(max_iter=5000).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC with Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_50 data set\n",
      "The accuracy of the predictions is: 0.81279\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.480687  0.061383  0.007252\n",
      "3          0.298283  0.555556  0.054131\n",
      "5          0.221030  0.383061  0.938617\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_100 data set\n",
      "The accuracy of the predictions is: 0.82561\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.547210  0.054390  0.007511\n",
      "3          0.246781  0.585859  0.053354\n",
      "5          0.206009  0.359751  0.939135\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_200 data set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-68ace219d8fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Training using doc2vec_{i} data set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_vali\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_vali\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_vali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "C = 1.0\n",
    "\n",
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = SVC(kernel='linear', C=C).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = SVC(kernel='rbf', gamma=0.7, C=C).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC with poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = SVC(kernel='poly', degree=3, gamma='auto', C=1.0).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_50 data set\n",
      "The accuracy of the predictions is: 0.77431\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.272532  0.020979  0.001036\n",
      "3          0.281116  0.352758  0.023569\n",
      "5          0.446352  0.626263  0.975395\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_100 data set\n",
      "The accuracy of the predictions is: 0.75472\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.148069  0.006216  0.000259\n",
      "3          0.257511  0.278943  0.013209\n",
      "5          0.594421  0.714841  0.986532\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_200 data set\n",
      "The accuracy of the predictions is: 0.72586\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.038627  0.002331  0.000000\n",
      "3          0.238197  0.188811  0.012173\n",
      "5          0.723176  0.808858  0.987827\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_count data set\n",
      "The accuracy of the predictions is: 0.75490\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.193133  0.003108  0.000259\n",
      "3          0.186695  0.238539  0.004921\n",
      "5          0.620172  0.758353  0.994820\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = RandomForestClassifier(n_estimators=100).fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datasets:\n",
    "    print(f\"Training using doc2vec_{i} data set\")\n",
    "    X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "    clf = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\").fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_vali)\n",
    "    evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression using feature selection via chi^2, MI\n",
    "Marginally increases performance. Possibly due to logisitc regression already weighting good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with COUNT features\u001b[0m\n",
      "The accuracy of the predictions is: 0.84230\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.665242  0.056860  0.007239\n",
      "3          0.207977  0.615023  0.053947\n",
      "5          0.126781  0.328117  0.938814\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "\n",
    "x2 = SelectKBest(chi2, k=2000)\n",
    "\n",
    "X_train_x2 = x2.fit_transform(X_train, Y_train)\n",
    "X_vali_x2 = x2.transform(X_vali)\n",
    "\n",
    "print('\\033[1m' + f\"Trained with COUNT features\" '\\033[0m')\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train_x2, Y_train)\n",
    "Y_pred = clf.predict(X_vali_x2)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrained with COUNT features\u001b[0m\n",
      "The accuracy of the predictions is: 0.80489\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.478632  0.061033  0.008790\n",
      "3          0.276353  0.518519  0.052223\n",
      "5          0.245014  0.420449  0.938987\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"200\")\n",
    "\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=80)\n",
    "mi.fit(X_train, Y_train)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_vali)\n",
    "\n",
    "print('\\033[1m' + f\"Trained with COUNT features\" '\\033[0m')\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train_mi, Y_train)\n",
    "Y_pred = clf.predict(X_test_mi)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest with feature selection, via chi^2, MI\n",
    "Increases performance by 2-4% with some k settings on the doc2vecs and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_count data set\n",
      "The accuracy of the predictions is: 0.79017\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.388412  0.017871  0.004403\n",
      "3          0.287554  0.445998  0.042217\n",
      "5          0.324034  0.536131  0.953380\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training using doc2vec_count data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "    \n",
    "x2 = SelectKBest(chi2, k=500)\n",
    "X_train_x2 = x2.fit_transform(X_train, Y_train)\n",
    "X_vali_x2 = x2.transform(X_vali)\n",
    "    \n",
    "clf = RandomForestClassifier(n_estimators=100).fit(X_train_x2, Y_train)\n",
    "Y_pred = clf.predict(X_vali_x2)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_50 data set\n",
      "The accuracy of the predictions is: 0.74279\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.075107  0.006993  0.000259\n",
      "3          0.343348  0.272727  0.019684\n",
      "5          0.581545  0.720280  0.980057\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training using doc2vec_50 data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"200\")\n",
    "\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=len())\n",
    "mi.fit(X_train, Y_train)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_vali)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100).fit(X_train_mi, Y_train)\n",
    "Y_pred = clf.predict(X_test_mi)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVMs with a reduced dataset, using feature selection chi2, MI. \n",
    "Lets us run SVM classifiers in reasonable time by using a reduced feature set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_100 data set\n",
      "The accuracy of the predictions is: 0.81350\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.405579  0.043512  0.004662\n",
      "3          0.313305  0.549340  0.044548\n",
      "5          0.281116  0.407148  0.950790\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can reduce the time?\n",
    "print(f\"Training using doc2vec_100 data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"100\")\n",
    "    \n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=70)\n",
    "mi.fit(X_train, Y_train)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_vali)\n",
    "    \n",
    "clf = LinearSVC(max_iter=10000).fit(X_train_mi, Y_train)\n",
    "Y_pred = clf.predict(X_test_mi)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_count data set\n",
      "The accuracy of the predictions is: 0.83221\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.660944  0.073038  0.011137\n",
      "3          0.227468  0.613831  0.063196\n",
      "5          0.111588  0.313131  0.925667\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training using doc2vec_count data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "\n",
    "x2 = SelectKBest(chi2, k=5000)\n",
    "X_train_x2 = x2.fit_transform(X_train, Y_train)\n",
    "X_vali_x2 = x2.transform(X_vali)\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0).fit(X_train_x2, Y_train)\n",
    "Y_pred = clf.predict(X_vali_x2)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_200 data set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-e9c070c5d066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_train_mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_test_mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_vali\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     return _estimate_mi(X, y, discrete_features, True, n_neighbors,\n\u001b[1;32m--> 448\u001b[1;33m                         copy, random_state)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[1;32m--> 289\u001b[1;33m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[1;32m--> 289\u001b[1;33m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx_discrete\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_discrete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmutual_info_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mx_discrete\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my_discrete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py\u001b[0m in \u001b[0;36mmutual_info_score\u001b[1;34m(labels_true, labels_pred, contingency)\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontingency\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[0mlabels_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_clusterings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[0mcontingency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontingency_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         contingency = check_array(contingency,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py\u001b[0m in \u001b[0;36mcontingency_matrix\u001b[1;34m(labels_true, labels_pred, eps, sparse)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot set 'eps' when sparse=True\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munique\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignoring\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \"\"\"\n\u001b[1;32m--> 303\u001b[1;33m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training using doc2vec_200 data set\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=50)\n",
    "mi.fit(X_train, Y_train)\n",
    "X_train_mi = mi.transform(X_train)\n",
    "X_test_mi = mi.transform(X_vali)\n",
    "\n",
    "clf = SVC(kernel='rbf', degree=3, gamma='auto', C=1.0).fit(X_train_mi, Y_train)\n",
    "Y_pred = clf.predict(X_test_mi)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try use some boosting models now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\alecy\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\alecy\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alecy\\anaconda3\\lib\\site-packages (from xgboost) (1.18.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_50 with XGBoost\n",
      "[16:03:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "The accuracy of the predictions is: 0.80905\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.480687  0.046620  0.006216\n",
      "3          0.309013  0.538462  0.054908\n",
      "5          0.210300  0.414918  0.938876\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_100 with XGBoost\n",
      "[16:03:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "The accuracy of the predictions is: 0.81600\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.487124  0.032634  0.005180\n",
      "3          0.281116  0.559441  0.053613\n",
      "5          0.231760  0.407925  0.941207\n",
      "\n",
      "\n",
      "\n",
      "Training using doc2vec_200 with XGBoost\n",
      "[16:04:24] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { nrounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "The accuracy of the predictions is: 0.80762\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.412017  0.035742  0.003367\n",
      "3          0.306867  0.519814  0.045325\n",
      "5          0.281116  0.444444  0.951308\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in datasets:\n",
    "    if(i != \"count\"):\n",
    "        print(f\"Training using doc2vec_{i} with XGBoost\")\n",
    "        X_train, X_vali, Y_train, Y_vali = preprocess(i)\n",
    "        clf = XGBClassifier(nrounds=100).fit(X_train, Y_train)\n",
    "        Y_pred = clf.predict(X_vali)\n",
    "        evaluate(Y_pred, Y_vali.values)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using doc2vec_count with XGBoost\n",
      "The accuracy of the predictions is: 0.82900\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.553648  0.037296  0.003367\n",
      "3          0.208155  0.554779  0.042994\n",
      "5          0.238197  0.407925  0.953639\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training using doc2vec_count with XGBoost\")\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "clf = XGBClassifier().fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_vali)\n",
    "evaluate(Y_pred, Y_vali.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try stacking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000),\n",
    "                MultinomialNB(),\n",
    "                #XGBClassifier(),\n",
    "                #SVC(kernel='linear', C=1.0, probability=True),\n",
    "                RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "titles = ['Logistic Regression',\n",
    "          'Multinomial NB',\n",
    "          #'XGBoost',\n",
    "          #'Linear SVC kernel',\n",
    "          'Forest']\n",
    "\n",
    "meta_classifier = LogisticRegression(solver='lbfgs')\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"100\")\n",
    "\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(title, \"Accuracy:\",clf.score(X_vali, Y_vali))\n",
    "    \n",
    "stacker.fit(X_train, Y_train)\n",
    "print('\\nStacker Accuracy:', stacker.score(X_vali, Y_vali))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random stacking sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000),\n",
    "                MultinomialNB(),\n",
    "                XGBClassifier(),\n",
    "                SVC(kernel='linear', C=1.0, probability=True),\n",
    "                SVC(kernel='rbf', gamma=0.7, C=1.0),\n",
    "                SVC(kernel='poly', degree=3, gamma='auto', C=1.0),\n",
    "                RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "titles = ['Logistic Regression',\n",
    "          'Multinomial NB',\n",
    "          'XGBoost',\n",
    "          'Linear SVC kernel',\n",
    "          'RBF SVC kernel',\n",
    "          'Poly SVC kernel',\n",
    "          'Forest']\n",
    "\n",
    "meta_classifier = LogisticRegression(solver='lbfgs')\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(title, \"Accuracy:\",clf.score(X_vali, Y_vali))\n",
    "    \n",
    "stacker.fit(X_train, Y_train)\n",
    "print('\\nStacker Accuracy:', stacker.score(X_vali, Y_vali))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000),\n",
    "                MultinomialNB(),\n",
    "                XGBClassifier(),\n",
    "                #SVC(kernel='linear', C=1.0, probability=True),\n",
    "                #SVC(kernel='rbf', gamma=0.7, C=C, probability=True),\n",
    "                SVC(kernel='poly', degree=3, gamma='auto', C=1.0, probability=True),\n",
    "                RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "titles = ['Logistic Regression',\n",
    "          'Multinomial NB',\n",
    "          'XGBoost',\n",
    "          #'Linear SVC kernel',\n",
    "          #'RBF SVC kernel',\n",
    "          'Poly SVC kernel',\n",
    "          'Forest']\n",
    "\n",
    "meta_classifier = LogisticRegression(solver='lbfgs')\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "\n",
    "X_train, X_vali, Y_train, Y_vali = preprocess(\"count\")\n",
    "\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(title, \"Accuracy:\",clf.score(X_vali, Y_vali))\n",
    "    \n",
    "stacker.fit(X_train, Y_train)\n",
    "print('\\nStacker Accuracy:', stacker.score(X_vali, Y_vali))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a vectoriser vs \"count\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the predictions is: 0.85340\n",
      "\n",
      "Confusion Matrix of Correctly Labeled Classes %'s\n",
      "\n",
      "Predicted         1         3         5\n",
      "Truths                                 \n",
      "1          0.628755  0.048951  0.003367\n",
      "3          0.208155  0.639472  0.044807\n",
      "5          0.163090  0.311577  0.951826\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_vali, Y_train, y_vali = train_test_split(train_data[\"review\"], train_data[\"rating\"], test_size=0.20, random_state=8579)\n",
    "\n",
    "X_train_txt, y_train = np.array(X_train), np.array(Y_train)\n",
    "X_test_txt, y_test = np.array(X_vali), np.array(y_vali)\n",
    "\n",
    "vectoriser = CountVectorizer(ngram_range=(1, 2), stop_words = 'english')\n",
    "vectoriser.fit(X_train_txt)\n",
    "X_train = vectoriser.transform(X_train_txt)\n",
    "X_test = vectoriser.transform(X_test_txt)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "evaluate(Y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitVectorised(ngram_range = (1,1), randomstate = 8579):\n",
    "    # Split the reviews and ratings\n",
    "    X_train, X_vali, Y_train, y_vali = train_test_split(train_data[\"review\"], train_data[\"rating\"], test_size=0.20, random_state=randomstate)\n",
    "    X_train_txt, y_train = np.array(X_train), np.array(X_train)\n",
    "    X_test_txt, y_test = np.array(X_vali), np.array(y_vali)\n",
    "        \n",
    "    # vectorise the reviews\n",
    "    vectoriser = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "    vectoriser.fit(X_train_txt)\n",
    "    X_train = vectoriser.transform(X_train_txt)\n",
    "    X_test = vectoriser.transform(X_test_txt)\n",
    "    \n",
    "    return X_train, X_test, Y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways:\n",
    "- SVC with kernel = linear better than rest\n",
    "- Countvectoriser() > sparse matrix given\n",
    "- better to use voting features\n",
    "- feature selection is potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
